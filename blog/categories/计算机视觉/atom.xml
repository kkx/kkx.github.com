<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 计算机视觉 | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/计算机视觉/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2012-10-19T22:39:42-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[用opencv检测物体中心点]]></title>
    <link href="http://kkx.github.com/blog/2012/10/09/yong-opencvjian-ce-wu-ti-zhong-xin-dian/"/>
    <updated>2012-10-09T12:35:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/10/09/yong-opencvjian-ce-wu-ti-zhong-xin-dian</id>
    <content type="html"><![CDATA[<p>图片中的物体探测一直是比较热的课题，通过物体的检测可以对图片自动生成标签以及分类，可以很好的用在图片检索的领域。在做的毕业设计(也是论文)就涉及到这方面的。今天看一篇叫<a href="http://www.robots.ox.ac.uk/~vilem/cvpr2009.pdf">Class-Speciﬁc Hough Forests for Object Detection</a>的state of the art，发现原来导师叫我用到的方法和这个差不多，只是增加了某些小trick用于对特定的数据集优化。</p>

<p>Hough Forests。 其实就是一个random forests 也就是随机森林,每条数据是一小块图片的description(patch) 树的每个节点对数据的分割通过information gain，努力把相同类的patch和相似的patch分到同一个枝头上去。每次通过数据集,随机采集部分features用于分割，每个这些features的分割点生成一个pool。然后对每个feature的分割点做一个信息量的检测，就像一个标准的random forests一样， 这里信息量的计算通过一下程式(这么说比较台湾腔 夯):
$$
Entropy({c_i}) = -c\cdot \log c -(1-c)\cdot \log(1-c)
$$
这里c就是patch是属于物体c的概率，1-c就是negative cases。o
简单吧，其实在原来的论文里还有一个东西，就是entropy只是衡量split的优差的其中一个feature，还有一个feature是offset也就是patch到中心点的距离，通过增加这个衡量点，相似距离的patch会在一起哦。测试的结果是，加上这个feature，precision会有很好的提高。
怎么计算这个feature，其实很简单，用Root mean square(RMS)把split后的patch到中心点的距离计算下，split之后小的RMS更优。到达叶子的数据，每个patch会有一次投票的机会给出中心点的位置。 </p>

<p>训练之后，每个测试图片会被分成不同的patch，通过hough forests 到达叶子的patch生成不同中心点的位置(根据在那叶子里的中心点patch). 最后，通过投票，某个物体的中心点的票数会很高，就这样我们就能找到一个物体的中心点了。 如果一个patch是不是物体的一部分，生成的中心点是错误的，但是中心点的位置其实是随机的，也就是说如果有很多很多非物体patch生成的map是没有一个明显的中心点的。而如果有很多物体中某部位的patch，生成出来的中心点是有一个很大的偏向性的，往往会往某一个中心聚拢。</p>

<p>这里有一个问题就是怎么计算一个patch到中心点的位置。首先训练数据一定得是每张图片只有一个物体。每个patch计算自己中心到物体的中心并保存x轴和y轴的移动。</p>

<p>道理很简单，给我的代码。
opencv这个库让我又爱又恨啊，很好用，很多很复杂的算法只要一行代码就ok了，但是python binding里有很多bug。而且很多错误你不一行行代码拆下来是不会看到的。。。
首先，不想使用论文里的hough forest，因为感觉上在opencv里你得自己hack下，不值得。第二貌似没找到patch的descriptor，所以在自己的实验里，只用到了特征点的descriptors，像sift和surf。使用这个的有点就是，可以做到orientation insensitive和scale insensitive。 其实，论文里的方法是没有能做到scale insensitive的，因为距离是个relative的东西，论文里，通过使用不同的scale进行测试，获得最优的结果作为最终结果。 而view insensitive更没有解决方法，按他们的方法，只有通过增加不同view的训练数据来弥补吧。</p>

<p>但是！导师教给我的方法能避免这个问题。首先sift和surf本身是view insensitive的，而在计算距离的时候通过对特征点本身的大小对距离做个归一化，这样就能让系统获得对不同大小的物体的识别能力。 
另外一个trick就是sift之类的特征点自己会带有gradient orientation,也就是梯度方向，这个信息是在计算距离的时候必须使用到的。因为相同descriptor的特征点可能会有不同的梯度方向，如果你只使用了图片原始的x和y轴保存距离的话，预测的中心点会偏离本来的实际位置。下面的图片就是个case: 
<img class="center" src="http://kkx.github.com/images/circle.png" width="680" height="420" title="梯度方向" ></p>

<p>我有一个圆圈，底部是白色的，如果是sift当特征点的检测的话，每个圆圈边上的点的特征值是一样的，而他们的梯度方向可能会不一样，如果不使用梯度方向的话，a作为训练的数据，而比作为测试数据，中心点预测的结果会在圆圈的外面(细线部分)。
如果要避免以上问题，只有通过<a href="http://en.wikipedia.org/wiki/Rotation_matrix">rotation matrix</a> 让每个特征点更具自己的梯度方向生成自己的x和y轴。 并且每个特征点的相对于中心点的距离要在新的x和y轴上进行计算。</p>

<ul>
  <li>
<script type="math/tex; mode=display"> x_{new} = x \cdot cos(angle) - y \cdot sin(angle) </script>
  </li>
  <li>
<script type="math/tex; mode=display"> y_{new} = x \cdot sin(angle) + y \cdot cos(angle) </script>
  </li>
</ul>

<p>如果需要从新轴返回到原始的x y轴只需要把angle乘以-1就可以了, 上面的angle是以radian为单位的。</p>

<p><img class="center" src="http://kkx.github.com/images/circle2.png" width="680" height="420" title="rotation matrix" >
假设上图是更具特征点的梯度方向做的x和y轴的转换，转换之后，可以发现点离圆圈的中心的距离不变，但是在x轴和y轴的移动发生了变化。</p>

<p>下面是测试方法的代码，包括训练和测试。数据集的话是使用<a href="http://cogcomp.cs.illinois.edu/Data/Car/">ucic</a>里的汽车数据，训练集里有positive cases 和negative cases，这里我只用到了positive cases。也没有是用到hough forest做训练，只用到了最近友邻。 每次选择最相近的友邻做出投票。在训练数据里，所有的车子中心点都在图的当中，因为每张图片里的车子几乎撑满整张图，而所有图片的尺寸都是40x100，在训练数据里，有两部分，一部分是没有scale变化的车子数据库，而另外一部分里车子的大小会有变化，用于测试方法对于scale变化的鲁棒性。 </p>

<p>下面是python代码</p>

<pre><code>import cv2
import math
import sys
import os
import numpy as np
kernel=np.array([[0,0,0,0,0,0,0,0.0002,0.0009,0.0014,0.0016,0.0014,0.0009,0.0002,0,0,0,0,0,0,0],[0,0,0,0,0,0.0005,0.0021,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0021,0.0005,0,0,0,0,0],[0,0,0,0.0000,0.0016,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0016,0.0000,0,0,0],[0,0,0.0000,0.0020,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0020,0.0000,0,0],[0,0,0.0016,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0016,0,0],[0,0.0005,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0005,0],[0,0.0021,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0021,0],[0.0002,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0002],[0.0009,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0009],[0.0014,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0014,],[0.0016,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0016],[0.0014,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0014],[0.0009,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0009],[0.0002,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0002],[0,0.0021,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0021,0],[0,0.0005,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0005,0,],[0,0,0.0016,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0016,0,0],[0,0,0.0000,0.0020,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0020,0.0000,0,0],[0,0,0,0.0000,0.0016,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0016,0.0000,0,0,0],[0,0,0,0,0,0.0005,0.0021,0.0031,0.0032,0.0032,0.0032,0.0032,0.0032,0.0031,0.0021,0.0005,0,0,0,0,0],[0,0,0,0,0,0,0,0.0002,0.0009,0.0014,0.0016,0.0014,0.0009,0.0002,0,0,0,0,0,0,0]])

FLANN_INDEX_KDTREE = 1  # bug: flann enums are missing
AUTOTUNED = 255
flann_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 4)


def get_key_points_from_img(img):
    if isinstance(img,str):
        img = cv2.imread(img)
    im = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    surf = cv2.SURF(1000, 4, 2, True)
    keypoints, descriptors = surf.detect(im, None, False)
    l_blue = cv2.cv.RGB(200, 200, 250)
    descriptors.shape = (-1, surf.descriptorSize())

    # y x
    center = (im.shape[0]/2,im.shape[1]/2)
    attributes = []

    for i in range(len(keypoints)):
        kp = keypoints[i]
        # (x y)
        scale = kp.size
        #cv2.circle(im, (int(kp.pt[0]), int(kp.pt[1])), int(kp.size/2), l_blue, 2, cv2.CV_AA)
        gradient = math.radians(kp.angle)

        #calculate the distances x and y in the new axis(rotated by the gradient)
        d_x = center[1]-kp.pt[0]
        d_y = center[0]-kp.pt[1]
        tx = math.cos(gradient) * d_x - math.sin(gradient) * d_y
        tx /= scale
        ty = math.sin(gradient) * d_x + math.cos(gradient) * d_y
        ty /= scale
        # generate data for further experiments
        attributes.append((kp.pt[0], kp.pt[1], scale, gradient, ty, tx, 0, 0, 0, 0, 1))
    return (descriptors,attributes)



if __name__ == "__main__":

    dir_path = "./CarData/TrainImages/"
    files = os.listdir(dir_path)

    #build train dataset
    train_descriptors = []
    train_attributes = []
    for f in files:
        if f.startswith("pos"):
            (descriptors, attributes) = get_key_points_from_img(dir_path+f)  
            train_descriptors.extend(descriptors) 
            train_attributes.extend(attributes)

    print len(train_descriptors)
    train_descriptors = np.array(train_descriptors)
    #build pnn model
    flann_index = cv2.flann_Index(train_descriptors, flann_params)

    dir_path = "./CarData/TrainImages/"
    dir_path = "./CarData/TestImages/"
    dir_path = "./CarData/TestImages_Scale/"

    files = os.listdir(dir_path)
    count1 = 0
    count2 = 0
    for f in files:
        img = cv2.imread(dir_path+f)
        (descriptors, attributes) = get_key_points_from_img(img)  
        im = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        mapa = np.zeros(im.shape, dtype=float)
        indexes, dist = flann_index.knnSearch(descriptors, 15, params={})  
        for i in range(len(descriptors)):
            for j in range(len(indexes[i])):
                scale = attributes[i][2]
                gradient = attributes[i][3]
                d_y = train_attributes[indexes[i][j]][4]*scale
                d_x = train_attributes[indexes[i][j]][5]*scale
                tx = math.cos(-gradient) * d_x - math.sin(-gradient) * d_y
                ty = math.sin(-gradient) * d_x + math.cos(-gradient) * d_y
                predicted_center_x = int(round(attributes[i][0]+tx))
                predicted_center_y = int(round(attributes[i][1]+ty))
                if (predicted_center_x&gt;=0) and (predicted_center_y&gt;=0) and (predicted_center_x&lt;mapa.shape[1]) and (predicted_center_y&lt;mapa.shape[0]):
                    mapa[predicted_center_y][predicted_center_x] += 1
                    count1 += 1
                else:
                    count2 += 1

        #apply a matlab disk filter to the output map for visualization
        mapa2 = cv2.filter2D(mapa,-1,kernel)*5
        cv2.imshow("original",im)
        cv2.imshow("image", mapa2)
        cv2.waitKey()
</code></pre>

<p>一大坨kernel变量，其实就是matlab的disk filter: fspecial(‘disk’, 20) opencv里就没有合适的filter可以用来平滑图片的!。 这里使用到了flann用来做最近友邻的模型，由于数据不是特别大，所以这里也没什么优化。在训练的时候，会把每个特征点距离到中心点的距离计算一下，计算完会根据特征点的大小把距离缩放一下，而当在测试的时候也会根据每个特征点的大小把距离放大。这样就能达到scale insensitive的目的了。 每个特征点会根据友邻做投票，投票的结果会映射到一个和图片一样大小的map上面。 通过filter2d过滤之后，会出现一大坨模糊的东西，哪里最亮哪里的投票数就最多，是中心点的可能性也最大。</p>

<p>下面就是一些实验图片的结果。
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>1.png" width="680" height="320" title="梯度方向" >
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>2.png" width="680" height="320" title="梯度方向" >
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>3.png" width="680" height="320" title="梯度方向" >
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>4.png" width="680" height="320" title="梯度方向" >
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>5.png" width="680" height="320" title="梯度方向" >
<img class="center" src="http://kkx.github.com/images/oct<em>9_t</em>6.png" width="680" height="320" title="梯度方向" ></p>

<p>貌似结果很好的，但是其实不是的，因为这里没用到false cases，会出现很多false positive，而且，由于是使用sift detector, 本身控制不了哪些特征点是需要的哪些是不需要的，在测试后的时候，如果测试图片的某个非物体部位的特征点很多的话(细节比较多)，会产生很多假票, 像最后一张图的两根柱子。在下次的试验力，一定得吧那些false cases包含到训练库里，当测试数据碰到的一个false case的票，直接把权值归0，这样的话应该能对结果有很大的改观。还有就是这里，每张投票的权值都是1，可以根据友邻的距离设置投票的权值，近的投票全职大，这样结果应该会好一些。 </p>

<p>opencv用起来真的没有matlab舒服，就连一个类似matlab的imregionalmax函数都没有，这个函数可以计算local maximum，这样的话，选取峰值最高的几个点作为预测结果会有很理想的结果。所以这套代码在这里也不能用来获得和真实中心点做比较。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 3 Perceptual Hash(phash) 图片指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen/"/>
    <updated>2012-09-10T17:03:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen</id>
    <content type="html"><![CDATA[<p>最后一个要介绍的不基于语义的图片指纹生成方法: pHash, 这里是pHash算法简要:</p>

<ul>
  <li>图片指纹灰值化</li>
  <li>图片缩放到32x32</li>
  <li>计算机视觉每个图片的Discrete Cosine Transform (DCT, type II)。</li>
  <li>只保留最左上部的8x8的DCT系数(有32x32大)，这样保存了频率最低的那部分(图片信息的大部分)</li>
  <li>计算中值</li>
  <li>生成64维2维码，0表示系数小于中值，反之为1.</li>
</ul>

<p>说到DCT，其实就想傅里叶转换一样的，把信号转换成很多不同频率和振幅的正玄曲线相加的结果。但是DCT只是用cosine函数。这样的话图片高频率部分会给擦去，只保留低频率部分。因为在给图片操作的时候，往往低频率的DCT系数会给保留，而且大部分图片的信息会给保留在这些低频率DCT系数里。(JPEG压缩也用这个方法来对图片进行压缩)。
DCT 会生成8x8的系数表，那最左上方的表示最低频率的元素，也是最重要的，越往右下的表示相对频率稍高的元素(好多信号出来的东西啊，头好大)
反正到最后能生成一个图片指纹，从别的作者的实验结果来看，效果是相对较好的。在这个网站上<a href="http://phash.org">phash.org</a>能下到开源的代码，在网站的demo上可以可以试试看计算图片的相似性，通过测试，只有DCT比较靠谱，另外两个的结果非常糟糕。相同的，这个算法的信息量为2^64，冲撞率应该不高，吧？</p>

<p><a href="http://phash.org">phash.org</a>网站上是用c/c++写的代码，没有python的binding，在<a href="https://github.com/polachok/py-phash/">https://github.com/polachok/py-phash/</a>可以找到一个简陋的python binding，里面有我们需要的DCT的计算方法，调用一个函数就能直接获得指纹数值。测试了几下，效果还不错。不过还是需要很大量的图片用语测试false positive的概率。
在github上能看到作者写的使用方法说明，我这里具体的代码如下:</p>

<pre><code>
import pHash
import sys

if __name__ == "__main__":
    hash1 = pHash.imagehash(sys.argv[1])
    hash2 = pHash.imagehash(sys.argv[2])
    print 'Hamming distance: %d (%08x / %08x)' % ( pHash.hamming_distance( hash1, hash2 ), hash1, hash2 )

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 2图片缩小指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen/"/>
    <updated>2012-09-09T00:09:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen</id>
    <content type="html"><![CDATA[<p>图片缩小之后其实保留个部分图片的信息，可以用来当做图片的指纹，在查找相关资料的时候发现了一个很不错的<a href="http://hackerlabs.org/blog/2012/07/30/organizing-photos-with-duplicate-and-similarity-checking/">博客</a>
在这个博客里，几乎所有的和图片相似性计算的算法都在了(除了基于语义的)。超级不错。里面的很多算法的实现是用pearl的，而本身抱着学习的态度我决定重新用python对其中的某些算法实现一遍。</p>

<p>这里是图片缩小指纹的算法:</p>

<ul>
  <li>缩放到160x160的图片</li>
  <li>灰值化</li>
  <li>模糊图片，把噪音去除掉</li>
  <li>equalize，让图片反差更大</li>
  <li>计算图片平均值</li>
  <li>缩放到16x16的图，并且降到2维，更具图片pixel值:大于平均值的为1，反之为0</li>
</ul>

<p>这样就生成了图片的指纹，信息总量有2^256, 哈希冲撞应该不大。
计算的图片相似度的时候直接使用xor。公式可以是这样: 1 - xor(p1,p2)/256</p>

<p>具体python实现代码如下:</p>

<pre><code>from PIL import Image
import sys
import ImageFilter
import ImageOps 
import numpy as np

def normalize(img, size = (160, 160)):
    return ImageOps.equalize(img.resize(size).convert('L').filter(ImageFilter.BLUR))

def calculate_fingerprint(img):
    img = normalize(img)
    mean_value = np.mean(np.array(img.getdata()))
    img = img.resize((16,16))
    return np.array(img.getdata())&gt;mean_value

def calculate_simility_by_path(img_path1, img_path2):
    img1 = Image.open(img_path1)
    img2 = Image.open(img_path2)
    fingerprint1 = calculate_fingerprint(img1) 
    fingerprint2 = calculate_fingerprint(img2) 
    return 1 - (0.0+np.sum(np.logical_xor(fingerprint1, fingerprint2)))/fingerprint1.size

if __name__ == "__main__":
    print calculate_simility_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>更好的implementation可以在这个<a href="http://www.ostertag.name/HowTo/findimagedupes.pl">链接</a>找到，虽然做法上不一样，但是原理应该都差不多。在代码的注释里，能看到这个类算法的优点和缺点，如果图片相似度较高的话，识别起来没什么问题。最大的缺点是，很多false positive，如果图片是一张大海的话 会出现一下情况</p>

<pre><code>1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
</code></pre>

<p>作者还说到对于大规模数据的话，性能会降低很多。但是貌似这个能用bloomfilter解决，现在先不管他</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 1颜色直方图差]]></title>
    <link href="http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai/"/>
    <updated>2012-09-08T15:30:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai</id>
    <content type="html"><![CDATA[<p>对于颜色直方图差的比对能很好的检测出图片之间的色差变化，那对于两张环境，光线，角度长不错的图片，能很好的检测出他们的相似度。
这是一个很老的方法，简单粗暴，非常使用。原理在这篇<a href="http://blog.csdn.net/lanphaday/article/details/2325027">博客</a>里很清楚的讲解了。 算法很简单:</p>

<ul>
  <li>把两张图片缩放到统一的大小，这里可以是256x256， 并且统一成rgb模式。</li>
  <li>统计每张图片的每个颜色的出现次数。那这里因为是rgb模式，有3*256=768 种颜色，生成直方图</li>
  <li>对两张图片的直方图进行比对。如果统计结果一致的话，相似度+1</li>
</ul>

<p>这里有个问题就是对空间信息的丢失，原作者通过把图片分割成16个小方格来解决这个问题。(split_image函数)</p>

<p>具体代码如下:</p>

<pre><code>from PIL import Image
import sys
def normalize(img, size = (256, 256)):
    return img.resize(size).convert('RGB')

def sim_hist(hist1, hist2):
    assert len(hist1) == len(hist2)
    return sum(1 - (0 if hist1 == hist2 else float(abs(hist1 - hist2))/max(hist1, hist2)) for hist1, hist2 in zip(hist1, hist2))/len(hist1)


def calc_similar_by_path(path_img1, path_img2):
    li = normalize(Image.open(path_img1))
    ri = normalize(Image.open(path_img2))
    return sum(sim_hist(l.histogram(), r.histogram()) for l, r in zip(split_image(li), split_image(ri))) / 16.0



def split_image(img, part_size = (64, 64)):
    w, h = img.size
    pw, ph = part_size
    assert w % pw == h % ph == 0
    return [img.crop((i, j, i+pw, j+ph)).copy() \
        for i in xrange(0, w, pw) \
        for j in xrange(0, h, ph)]

if __name__ == "__main__":
    print calc_similar_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>这里的fingerprint就是每个图片的直方图。 这个指纹的信息量的话应该挺大的(至少有2^768) 理论上来讲应该false positive的可能性很小，但是这个结论应该是错的，因为在图片里很多颜色是出现的概率是很小的，大部分颜色都集中于某些值中。</p>

<p>代码和原作者的差不多，实验结果是不错的，很简单，但是这个算法有个致命的问题就是对颜色的过分依赖。对于图片角度的变化的容忍度挺高，但是如果颜色或者图片光线变化稍大就不能很好的检测出相似度了，会把相似的图片判断为不同的。最简单的方法就是弄个灰度图，这方法基于rgb的，就无解了。还有个小问题就是，false positive，我觉得如果两张白底黑字的文字图片，不管字是多么不一样，但是他们的相似度应该很高(在有的应用上，这个结果是很希望的)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: Introduction]]></title>
    <link href="http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction/"/>
    <updated>2012-09-08T14:18:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction</id>
    <content type="html"><![CDATA[<p>在db实习了2个多月很开心。老实说，在那里做的项目没有想象中的理想，觉得惭愧啊。前段时间清风老师有和我提起一个敏感图片检测的需求。当时候有试着用SIFT和min-hash实现，但是结果不是很理想。在公司和在家自己弄有很大的区别，时间上的紧迫，很多时候虽然头不会催，但是你看着别人在交东西，自己却没有，压力很大，很多时候就需要move on。结果就是这个项目没有完成, 感脚好对不住清风老师啊！真的不会再爱了！</p>

<p>这几天闲下来，我突然有想好好研究研究这方面的东西了，看了些paper，觉得有很多实现方法，结果有好的有坏的，我准备都试着去实现下，把他们的有点和缺点都记录下来，希望能找到一个很好的解决方案，如果运气好的话。</p>

<p>Near-duplicate image(高相似图片)的检测不比完全一样的图片检测来的简单，后者可以直接用哈希生成像MD5类似的指纹，然后保存每个图片的时候也保存那个指纹，这样在查找的时候只要比对指纹就可以了，这样的话速度上会有很大的提升。</p>

<p>那用什么样的办法能有效的比对图片的相似度呢? 方法有很多。 首先要说的一点是，在比对的时候，速度是非常重要的，所以，一般都是通过指纹(fingerprint)技术把一张图片合理的压缩成一个容量占用很小的方便计算相似度的数据集。 前面说过md5是不能用在这里的，为什么呢？因为一个微小的变化会是两个图片之间的MD5完全不一样。而在这里要做的是:</p>

<ul>
  <li>相同图片的指纹要一样</li>
  <li>类似图片的指纹也要类似</li>
  <li>完全不相同的图片指纹的差别很大</li>
</ul>

<p>做到以上几点，那么图片的相似度的识别就完成了，但是要找到一个函数f做到以上这种方法很难。这也是这里要慢慢探索的。</p>

]]></content>
  </entry>
  
</feed>
