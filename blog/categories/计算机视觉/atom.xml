<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 计算机视觉 | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/计算机视觉/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2012-09-16T13:45:06-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 3 Perceptual Hash(phash) 图片指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen/"/>
    <updated>2012-09-10T17:03:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen</id>
    <content type="html"><![CDATA[<p>最后一个要介绍的不基于语义的图片指纹生成方法: pHash, 这里是pHash算法简要:</p>

<ul>
  <li>图片指纹灰值化</li>
  <li>图片缩放到32x32</li>
  <li>计算机视觉每个图片的Discrete Cosine Transform (DCT, type II)。</li>
  <li>只保留最左上部的8x8的DCT系数(有32x32大)，这样保存了频率最低的那部分(图片信息的大部分)</li>
  <li>计算中值</li>
  <li>生成64维2维码，0表示系数小于中值，反之为1.</li>
</ul>

<p>说到DCT，其实就想傅里叶转换一样的，把信号转换成很多不同频率和振幅的正玄曲线相加的结果。但是DCT只是用cosine函数。这样的话图片高频率部分会给擦去，只保留低频率部分。因为在给图片操作的时候，往往低频率的DCT系数会给保留，而且大部分图片的信息会给保留在这些低频率DCT系数里。(JPEG压缩也用这个方法来对图片进行压缩)。
DCT 会生成8x8的系数表，那最左上方的表示最低频率的元素，也是最重要的，越往右下的表示相对频率稍高的元素(好多信号出来的东西啊，头好大)
反正到最后能生成一个图片指纹，从别的作者的实验结果来看，效果是相对较好的。在这个网站上<a href="http://phash.org">phash.org</a>能下到开源的代码，在网站的demo上可以可以试试看计算图片的相似性，通过测试，只有DCT比较靠谱，另外两个的结果非常糟糕。相同的，这个算法的信息量为2^64，冲撞率应该不高，吧？</p>

<p><a href="http://phash.org">phash.org</a>网站上是用c/c++写的代码，没有python的binding，在<a href="https://github.com/polachok/py-phash/">https://github.com/polachok/py-phash/</a>可以找到一个简陋的python binding，里面有我们需要的DCT的计算方法，调用一个函数就能直接获得指纹数值。测试了几下，效果还不错。不过还是需要很大量的图片用语测试false positive的概率。
在github上能看到作者写的使用方法说明，我这里具体的代码如下:</p>

<pre><code>
import pHash
import sys

if __name__ == "__main__":
    hash1 = pHash.imagehash(sys.argv[1])
    hash2 = pHash.imagehash(sys.argv[2])
    print 'Hamming distance: %d (%08x / %08x)' % ( pHash.hamming_distance( hash1, hash2 ), hash1, hash2 )

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 2图片缩小指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen/"/>
    <updated>2012-09-09T00:09:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen</id>
    <content type="html"><![CDATA[<p>图片缩小之后其实保留个部分图片的信息，可以用来当做图片的指纹，在查找相关资料的时候发现了一个很不错的<a href="http://hackerlabs.org/blog/2012/07/30/organizing-photos-with-duplicate-and-similarity-checking/">博客</a>
在这个博客里，几乎所有的和图片相似性计算的算法都在了(除了基于语义的)。超级不错。里面的很多算法的实现是用pearl的，而本身抱着学习的态度我决定重新用python对其中的某些算法实现一遍。</p>

<p>这里是图片缩小指纹的算法:</p>

<ul>
  <li>缩放到160x160的图片</li>
  <li>灰值化</li>
  <li>模糊图片，把噪音去除掉</li>
  <li>equalize，让图片反差更大</li>
  <li>计算图片平均值</li>
  <li>缩放到16x16的图，并且降到2维，更具图片pixel值:大于平均值的为1，反之为0</li>
</ul>

<p>这样就生成了图片的指纹，信息总量有2^256, 哈希冲撞应该不大。
计算的图片相似度的时候直接使用xor。公式可以是这样: 1 - xor(p1,p2)/256</p>

<p>具体python实现代码如下:</p>

<pre><code>from PIL import Image
import sys
import ImageFilter
import ImageOps 
import numpy as np

def normalize(img, size = (160, 160)):
    return ImageOps.equalize(img.resize(size).convert('L').filter(ImageFilter.BLUR))

def calculate_fingerprint(img):
    img = normalize(img)
    mean_value = np.mean(np.array(img.getdata()))
    img = img.resize((16,16))
    return np.array(img.getdata())&gt;mean_value

def calculate_simility_by_path(img_path1, img_path2):
    img1 = Image.open(img_path1)
    img2 = Image.open(img_path2)
    fingerprint1 = calculate_fingerprint(img1) 
    fingerprint2 = calculate_fingerprint(img2) 
    return 1 - (0.0+np.sum(np.logical_xor(fingerprint1, fingerprint2)))/fingerprint1.size

if __name__ == "__main__":
    print calculate_simility_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>更好的implementation可以在这个<a href="http://www.ostertag.name/HowTo/findimagedupes.pl">链接</a>找到，虽然做法上不一样，但是原理应该都差不多。在代码的注释里，能看到这个类算法的优点和缺点，如果图片相似度较高的话，识别起来没什么问题。最大的缺点是，很多false positive，如果图片是一张大海的话 会出现一下情况</p>

<pre><code>1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
</code></pre>

<p>作者还说到对于大规模数据的话，性能会降低很多。但是貌似这个能用bloomfilter解决，现在先不管他</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 1颜色直方图差]]></title>
    <link href="http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai/"/>
    <updated>2012-09-08T15:30:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai</id>
    <content type="html"><![CDATA[<p>对于颜色直方图差的比对能很好的检测出图片之间的色差变化，那对于两张环境，光线，角度长不错的图片，能很好的检测出他们的相似度。
这是一个很老的方法，简单粗暴，非常使用。原理在这篇<a href="http://blog.csdn.net/lanphaday/article/details/2325027">博客</a>里很清楚的讲解了。 算法很简单:</p>

<ul>
  <li>把两张图片缩放到统一的大小，这里可以是256x256， 并且统一成rgb模式。</li>
  <li>统计每张图片的每个颜色的出现次数。那这里因为是rgb模式，有3*256=768 种颜色，生成直方图</li>
  <li>对两张图片的直方图进行比对。如果统计结果一致的话，相似度+1</li>
</ul>

<p>这里有个问题就是对空间信息的丢失，原作者通过把图片分割成16个小方格来解决这个问题。(split_image函数)</p>

<p>具体代码如下:</p>

<pre><code>from PIL import Image
import sys
def normalize(img, size = (256, 256)):
    return img.resize(size).convert('RGB')

def sim_hist(hist1, hist2):
    assert len(hist1) == len(hist2)
    return sum(1 - (0 if hist1 == hist2 else float(abs(hist1 - hist2))/max(hist1, hist2)) for hist1, hist2 in zip(hist1, hist2))/len(hist1)


def calc_similar_by_path(path_img1, path_img2):
    li = normalize(Image.open(path_img1))
    ri = normalize(Image.open(path_img2))
    return sum(sim_hist(l.histogram(), r.histogram()) for l, r in zip(split_image(li), split_image(ri))) / 16.0



def split_image(img, part_size = (64, 64)):
    w, h = img.size
    pw, ph = part_size
    assert w % pw == h % ph == 0
    return [img.crop((i, j, i+pw, j+ph)).copy() \
        for i in xrange(0, w, pw) \
        for j in xrange(0, h, ph)]

if __name__ == "__main__":
    print calc_similar_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>这里的fingerprint就是每个图片的直方图。 这个指纹的信息量的话应该挺大的(至少有2^768) 理论上来讲应该false positive的可能性很小，但是这个结论应该是错的，因为在图片里很多颜色是出现的概率是很小的，大部分颜色都集中于某些值中。</p>

<p>代码和原作者的差不多，实验结果是不错的，很简单，但是这个算法有个致命的问题就是对颜色的过分依赖。对于图片角度的变化的容忍度挺高，但是如果颜色或者图片光线变化稍大就不能很好的检测出相似度了，会把相似的图片判断为不同的。最简单的方法就是弄个灰度图，这方法基于rgb的，就无解了。还有个小问题就是，false positive，我觉得如果两张白底黑字的文字图片，不管字是多么不一样，但是他们的相似度应该很高(在有的应用上，这个结果是很希望的)</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: Introduction]]></title>
    <link href="http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction/"/>
    <updated>2012-09-08T14:18:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction</id>
    <content type="html"><![CDATA[<p>在db实习了2个多月很开心。老实说，在那里做的项目没有想象中的理想，觉得惭愧啊。前段时间清风老师有和我提起一个敏感图片检测的需求。当时候有试着用SIFT和min-hash实现，但是结果不是很理想。在公司和在家自己弄有很大的区别，时间上的紧迫，很多时候虽然头不会催，但是你看着别人在交东西，自己却没有，压力很大，很多时候就需要move on。结果就是这个项目没有完成, 感脚好对不住清风老师啊！真的不会再爱了！</p>

<p>这几天闲下来，我突然有想好好研究研究这方面的东西了，看了些paper，觉得有很多实现方法，结果有好的有坏的，我准备都试着去实现下，把他们的有点和缺点都记录下来，希望能找到一个很好的解决方案，如果运气好的话。</p>

<p>Near-duplicate image(高相似图片)的检测不比完全一样的图片检测来的简单，后者可以直接用哈希生成像MD5类似的指纹，然后保存每个图片的时候也保存那个指纹，这样在查找的时候只要比对指纹就可以了，这样的话速度上会有很大的提升。</p>

<p>那用什么样的办法能有效的比对图片的相似度呢? 方法有很多。 首先要说的一点是，在比对的时候，速度是非常重要的，所以，一般都是通过指纹(fingerprint)技术把一张图片合理的压缩成一个容量占用很小的方便计算相似度的数据集。 前面说过md5是不能用在这里的，为什么呢？因为一个微小的变化会是两个图片之间的MD5完全不一样。而在这里要做的是:</p>

<ul>
  <li>相同图片的指纹要一样</li>
  <li>类似图片的指纹也要类似</li>
  <li>完全不相同的图片指纹的差别很大</li>
</ul>

<p>做到以上几点，那么图片的相似度的识别就完成了，但是要找到一个函数f做到以上这种方法很难。这也是这里要慢慢探索的。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[音乐乐器声音分类]]></title>
    <link href="http://kkx.github.com/blog/2012/05/21/yin-le-le-qi-sheng-yin-fen-lei/"/>
    <updated>2012-05-21T23:07:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/05/21/yin-le-le-qi-sheng-yin-fen-lei</id>
    <content type="html"><![CDATA[<p>音乐分类其实很早就有人做了，这些天在kaggle上有个<a href="http://www.kaggle.com/c/msdchallenge">Million Song Dataset Challenge</a> 非常火，上面有大量的数据可以让选手们通过底层，高层的和基于用户协作推荐的等等手段来对歌曲进行推荐. 这里我做的音乐声音分类是底层的, 而且我觉得一般的音乐文件要处理的东西太多了，节奏，速度什么的，在这里我只对4种乐器的声音进行分类: 钢琴，吉他，口琴，小提琴。</p>

<p>一般底层的声音，语音分类，speech recognition 使用无外乎<a href="MFCC">Mel-frequency cepstrum</a> 和 lpc analysis。 第二是个比较老的方法，而MFCC是我所用到的。可以把MFCCs对于好分类和识别的家伙们来说可以理解为对一段声音文件的descriptor，重点是这个descriptor非常discriminant. 在实验中，使用了matlab的voicebox里的melcepst函数，可以生成长度为12的数组，每个数组一般是一很小段声音的description，由于取样和设置的问题，MFCC所包含的声音长度是不一样的，在我的实验中，差不多连续1000个MFCC差不多是6秒钟的音频。 那4种声音来源分别是verycd上名家solo哈，不过要吐槽的是找了好多title是solo的，但是其实是是几种乐器和音的。每个类里取了6首左右的歌曲(20分钟左右)作为训练数据，提取出500k的MFCCs。</p>

<p>在实验中我使用了两个方法, 一种是在学校里老师讲的”常规”方法, 还有一种是用于图像分类的。总觉得在做底层分类的时候，图像识别和声音识别的步骤是差不多的。</p>

<ul>
  <li>Binary-split, SVM</li>
  <li>Naive Bayes Nearest Neighbor </li>
</ul>

<h2 id="binary-split--svm">Binary-split + SVM</h2>

<p>500k+的数据直接用来使用的话那就太夸张了，Binary-split+SVM这方法简单的来说是通过binary-split把500k的MFCC压缩成1000到2000个clusters，通过binary-split类似的数据会在一个小组里面。这种clustering算法里最有名的是k-means，不过k-means 速度实在是太悲剧了，虽然压缩损耗度要比binary-split少一些。 </p>

<p>通过binary-split数据给压缩到少许clusters里，这所有的clusters其实就是所谓的<a href="http://en.wikipedia.org/wiki/Codebook">codebook</a>。codebook在通讯里面非常用,往往在数据通讯里，特别是声音传输中，需要用到codebook把传输数据进行压缩，在接受点的那一端，通过decode来还原原始数据。像一般的手机通讯就是用到这个概念，所以说很多时候人们发现电话里的声音总不是特别像某个人，原因就是因为声音信息在传输的时候多少给损耗掉了。在实验中，每个声音段(通常是1000个MFCCs也就是6秒左右的声音)用codebook可以生成一个一个histogram: 把每个MFCC在binary-split树里跑一下，找到最相近的cluster，通过统计所有cluster里一个声音段落里不同的MFCCs所出现的次数来生成Histogram。</p>

<p>每个声音段所产生的histrogram就是所要用到用来训练SVM的数据。这里的svm是one-vs-all，所以说将有4个svm会训练出来，每个svm会应对每个类，而每个svm会长生出一个概率值，是只一个声音段是它所对应类的可能性。在测试中往往选取最高的可能性作为音频的class。</p>

<p>在测试中，以上的步骤都差不多，只是，不需要生成codebook，直接使用了就可以。这里要说的是，测试的音频长度不一定需要和训练的音频长度一样，只要每次生成的histogram是归一化的就行了。</p>

<table border="1" cellspacing="30" cellpadding="100">
    <tr>
    <th />
    <th />
    <th> Piano|</th>
    <th>          </th>
    <th> Guitar| </th>
    <th>          </th>
    <th> Violin| </th>
    <th />
    <th> Harmonica| </th>
    </tr>

    <tr>
    <th>Piano|</th>
    <th />
    <th>16</th>
    <th />
    <th>8</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Guitar|</th>
    <th />
    <th>4</th>
    <th />
    <th>12</th>
    <th />
    <th>20</th>
    <th />
    <th>4</th>
    </tr>
    
    <tr>
    <th>Violin|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    <th />
    <th>4</th>
    </tr>

    <tr>
    <th>Harmonica|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>16</th>
    </tr>
</table>

<p><br />
测试的时候每个类有20个case，从结果上来看，钢琴和吉他的识别不是很高，而且他们彼此有着很高互指。其实很大的原因是:当我们用codebook的时候，会对数据进行压缩，很多时候这个压缩会让比较discriminant的数据丢失。在这个实验中，钢琴和吉他的声音是比较相似的。</p>

<p><img class="center" src="http://kkx.github.com/images/histogram1.png" width="650" height="850">
上面这个图是个histogram的分布图，通过随机选取每个类里的某一个训练的音频段生成的分布图，从途中可以看出，实际上通过codebook之后，吉他和钢琴的数据分布是非常耦合和相似的。所以，实验中对钢琴和吉他的分别度不高也有了依据。</p>

<h2 id="naive-bayes-nearest-neighbornbnn">Naive Bayes Nearest Neighbor(NBNN)</h2>
<p>实际上，在图像识别中，很多时候也是运用到了上面所讲的方法，通过codebook对图像所提取的key-points进行clusterization，之后通过第二层的分类器把codebook产出的histogram数组进行分类。所以我觉得NBNN应该也能运用到声音分类中来。 NBNN是最近以篇叫<a href="http://www.google.es/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CFMQFjAA&amp;url=http%3A%2F%2Fwww.wisdom.weizmann.ac.il%2F~irani%2FPAPERS%2FInDefenceOfNN_CVPR08.pdf&amp;ei=d7G6T6yCKaOu0QX5_cD0Aw&amp;usg=AFQjCNGWk0lo_L6AEjgco6PPMUZ0YE-zOQ&amp;sig2=ZvwoYRICPitYbrkseMV8Bw">In Defense of Nearest-Neighbor Based Image Classification</a>的论文中看到的，该论文着重指出，codebook对数据损耗在无训练模型中的影响。 NBNN就是无训练模型(也叫no parametric?)， 优点就是不需要训练。。。还有一个优点是在分类里，NBNN用的是image-class距离，而不是image-image距离，这样的话，intravariance很高的类对它的影响不大。</p>

<p>在论文中如果我们使用的邻近邻居是1的话，算法简化成以下:</p>

<ol>
  <li>Compute MFCCs <script type="math/tex">d_i,...,d_n</script> of the query audio</li>
  <li><script type="math/tex">\forall d_i \forall C</script> compute the NN of <script type="math/tex">d_i</script> in <script type="math/tex">C</script>: <script type="math/tex">NN_C(d_i)</script></li>
  <li><script type="math/tex">C = arg min_c\sum_{i=1}^n</script> <script type="math/tex">\left \|d_i - NN_c(d_i)\right \|^2  </script></li>
</ol>

<p><img class="right" src="http://kkx.github.com/images/kd_tree.png" width="200" height="250" title="kt-tree" >
还有一点要说的是，在寻找最近邻居的时候，计算需求是相当高的，在原文里，作者用的是<a href="http://en.wikipedia.org/wiki/K-d_tree">kd-tree</a>,这样的能很快的找到最近的邻居。在这个实验的时候我用的scipy里kd-tree效率非常慢，后来才发现原来原作者是用的approximate kd-tree. </p>

<p>这个算法虽然简单，但是结果是惊人的不错:
使用所有的训练数据里的mfcc，每个测试音频有2000个MFCC，测试过程相当慢(好几个小时,如果使用approximate kd-tree的话应该能快好几个数量级)</p>

<table border="1" cellspacing="30" cellpadding="100">
    <tr>
    <th />
    <th />
    <th> Piano|</th>
    <th>          </th>
    <th> Guitar| </th>
    <th>          </th>
    <th> Violin| </th>
    <th />
    <th> Harmonica| </th>
    </tr>

    <tr>
    <th>Piano|</th>
    <th />
    <th>20</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Guitar|</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    <th />
    <th>1</th>
    <th />
    <th>0</th>
    </tr>
    
    <tr>
    <th>Violin|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>19</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Harmonica|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    </tr>
</table>

<p><br />
从数据集里随机在每个类中获取500个MFCC，每个音频400个MFCC</p>

<table border="1" cellspacing="30" cellpadding="100">
    <tr>
    <th />
    <th />
    <th> Piano|</th>
    <th>          </th>
    <th> Guitar| </th>
    <th>          </th>
    <th> Violin| </th>
    <th />
    <th> Harmonica| </th>
    </tr>

    <tr>
    <th>Piano|</th>
    <th />
    <th>20</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Guitar|</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    <th />
    <th>3</th>
    <th />
    <th>0</th>
    </tr>
    
    <tr>
    <th>Violin|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>17</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Harmonica|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    </tr>
</table>

<p><br />
最后是从每个类的训练数据中随机抽取30个MFCCs，每200个MFCC做音频测试数据的长度,测试数据是每个类40个，总使用的时间几秒而已</p>

<table border="1" cellspacing="30" cellpadding="100">
    <tr>
    <th />
    <th />
    <th> Piano|</th>
    <th>          </th>
    <th> Guitar| </th>
    <th>          </th>
    <th> Violin| </th>
    <th />
    <th> Harmonica| </th>
    </tr>

    <tr>
    <th>Piano|</th>
    <th />
    <th>35</th>
    <th />
    <th>5</th>
    <th />
    <th>2</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Guitar|</th>
    <th />
    <th>4</th>
    <th />
    <th>35</th>
    <th />
    <th>1</th>
    <th />
    <th>0</th>
    </tr>
    
    <tr>
    <th>Violin|</th>
    <th />
    <th>1</th>
    <th />
    <th>0</th>
    <th />
    <th>37</th>
    <th />
    <th>0</th>
    </tr>

    <tr>
    <th>Harmonica|</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>0</th>
    <th />
    <th>20</th>
    </tr>
</table>
<p><br /> 
结果很显然易见，NBNN在声音分类方面还是很厉害的。更证明了声音分类和图像分类的相似之处。 
以后有时间的话，可以试试以下的东西:</p>

<ul>
  <li>可以在NBNN里不随机选取MFCC，而是选取更有代表性的</li>
  <li>使用 approximate kd-tree</li>
  <li>用图像分类里的 random forest算法试试看</li>
</ul>

<p>最后，感谢电驴，感谢python，感谢scipy，感谢voicebox 让哥在3天里搞定大作业。感谢尼玛操蛋的htk，要不是因为htk(本来好想做也语音编程系统的的)出不来结果我也不会换做这个来当speech recognition的大作业。</p>

]]></content>
  </entry>
  
</feed>
