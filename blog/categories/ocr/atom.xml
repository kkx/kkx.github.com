<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ocr | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/ocr/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2013-06-09T02:23:30-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[tesseract-ocr 中文字符识别速度慢的问题]]></title>
    <link href="http://kkx.github.com/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti/"/>
    <updated>2012-07-08T04:01:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti</id>
    <content type="html"><![CDATA[<p>在做这个ocr项目的时候，首先想到的是用一些开源的项目，通过比较，只有google对中文的支持度算是有好的，通过测试，它的识别度也还是可以。但是会面临到好几个问题,可以察觉到2个主要问题</p>

<ul>
  <li>中文字相比英语字符，识别速度低了好几倍</li>
  <li>中文字符有时候会整行出现大面积的错误，而且非常离谱，甚至有乱码的出现。</li>
</ul>

<p>通过这篇<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/35248.pdf">Adapting the Tesseract Open Source OCR Engine for Multilingual OCR</a>的论文里看，这个项目的框架为了能让各种不同语言的文字进行识别，把中文文字识别以一行句子为切分单位，以文字部首为最基本的识别单位(使用了connected components analysis切分所有不连在一起的部首(也可以用项目的图形debug界面http://code.google.com/p/tesseract-ocr/wiki/ViewerDebugging，能看到中文字是给切分开的)。识别之后，通过best first search对中文字符进行匹配。结果就是，文字识别速度较慢，大部分计算的开销用在了匹配的时候。由于用到了best-first 不能保证全局最优解，往往图像稍微有一些噪音就会会出现大面积的错误的情况，一开始的几个字也许和原本字符相似度还可以，句子后半部的字符就不是了。</p>

<p>这个方法完全是按英语的识别方法，通过以英语单词为基本匹配单位，匹配每个字母成为最有可能的单词。这样的一个方法可以让语言的semantic运用到识别了，大大提高的每个单词的识别度。但是这个方法的本身是特别适合语英语，每个单词的长度不是很大，单词和单词之间有一个明显的空格，可以很好的切分。 但是运用到中文字符识别的时候确出现了以下几个问题:</p>

<ul>
  <li>以google那些人的看法，中文字符之间空隙是很小的，甚至很多时候是字和字之间是连在一起。其实印刷体的中文字符之间的空隙还是很明显的。</li>
  <li>中文句子长度一般都很长，往往匹配的时候需要很多开销</li>
  <li>中文识别的时候，我试过，就算把文本字符之间的空隙分的很大，还是会把一个字的之间的部首分开，在匹配的时候不会记住字符之间的空隙。</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[生成中文字图片]]></title>
    <link href="http://kkx.github.com/blog/2012/06/28/sheng-cheng-zhong-wen-zi-tu-pian/"/>
    <updated>2012-06-28T17:17:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/06/28/sheng-cheng-zhong-wen-zi-tu-pian</id>
    <content type="html"><![CDATA[<p>最近在做一个中文ocr，感觉是个不可完成任务。不管怎么样还是得试试。
首先要做的就是获得训练数据。中文的数据不比英文，量大而且没有地方下载，唯一能做的就是自己生成，在网上找了写函数自己改改就成了下面的脚本了</p>

<p>首先是一个中文字符集编码的生成函数:</p>

<pre><code>
    def generate_ch():

        #Function which generate all chinese gb2312 characters 
        
        body = random.randint(0xA, 0xA)
        tail = random.randint(0, 0xF)
        chars = []
        
        heads = range(0xB0, 0xF8)
        bodys = range(0xA, 0xF+1)
        tails = range(0, 0xF+1)
        for head in heads:
            for body in bodys:
                for tail in tails:
                    val = ( head &lt;&lt; 0x8 ) | (body &lt;&lt; 0x4 ) | tail
                    #print "%x"%head,"%x"%body,"%x"%tail,"%x"%val
                    str = "%x" % val
                    if str.endswith("ff") or str.endswith("a0"):
                        continue
                    #special cases no characters with these codecs
                    if str == "d7fa" or str == "d7fb" or str == "d7fc" or str == "d7fd" or str == "d7fe" or str == "d7ff" :
                        continue
                    chars.append(str.decode('hex').decode('gb2312'))
        return chars

</code></pre>

<p>如果要生成中文字符图片，有很多办法，这里我用pygame打印每个字符</p>

<pre><code>    # -*- coding: utf-8 -*-
    import pygame
     pygame.init()
    font = pygame.font.Font(os.path.join("fonts", "simfang.ttf"), 64)
    rtext = font.render("".join("b1a1".decode('hex').decode('gb2312')), True, (0, 0, 0), (255, 255, 255))
    pygame.image.save(rtext,"t.jpg" )
</code></pre>

<p>然后中文字符图片就出来了
<img class="center" src="http://kkx.github.com/images/character.jpg" width="300" height="300" title="原图" ></p>

<p>要打印字符，需要至少一个ttf,也就是字体的文件。这里是用的simfang.ttf</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gabor filters对中文字图片的特征提取]]></title>
    <link href="http://kkx.github.com/blog/2012/06/28/gabor-filtersdui-zhong-wen-zi-tu-pian-de-te-zheng-ti-qu/"/>
    <updated>2012-06-28T16:52:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/06/28/gabor-filtersdui-zhong-wen-zi-tu-pian-de-te-zheng-ti-qu</id>
    <content type="html"><![CDATA[<p>中文字ocr是个比较讨厌的事情，主要是因为中文字繁多，光简体字就有6000多个以上，虽然其中大部分简体字是不常用的。对于这么一个这么大的，种类繁多的数据集，在分类的时候真心头疼，看了很多paper后，发现现在的state of the art是基于gabor filters，通过这个过滤器，对一个中文字的不同角度的比划进行过滤，然后在每个subregion里的每个角度的比划进行统计之后生成一个histogram，和bag of words一样，histogram可以用来对每个字进行分类或者用来寻找最近邻。</p>

<p>gabor filters 是个复杂的东西，网上的方程各种各样的，数学不好的小白就很难明白。在网上找了好多现成的函数之后，最后找到一个能用的matlab函数:</p>

<pre><code>
function gb=gabor_fn(rows,cols,sigma,theta,lambda,psi,gamma)
     
    sigma_x = sigma;
    sigma_y = sigma/gamma;
     
    % Bounding box
    nstds = 3;
    xmax = max(abs(nstds*sigma_x*cos(theta)),abs(nstds*sigma_y*sin(theta)));
    xmax = ceil(max(1,xmax));
    ymax = max(abs(nstds*sigma_x*sin(theta)),abs(nstds*sigma_y*cos(theta)));
    ymax = ceil(max(1,ymax));
    xmin = -xmax; ymin = -ymax;

    [x,y] = meshgrid(xmin:xmax,ymin:ymax);
    %[x,y] = meshgrid(-rows/2:rows/2-1,-cols/2:cols/2-1);

    % Rotation 
    x_theta=x*cos(theta)+y*sin(theta);
    y_theta=-x*sin(theta)+y*cos(theta);
    gb= exp(-.5*(x_theta.^2/sigma_x^2+y_theta.^2/sigma_y^2)).*cos(2*pi/lambda*x_theta+psi);
</code></pre>

<p>rows和cols是图片的大小, sigma是gauss的那个sigma，越大的话周围的pixels影响越大,theta要提取的比划的角度，lambda这个貌似是什么的长度(不明白啊),psi这里没用,gamma生成y_sigma。</p>

<p>通过改变theta这个角度值可以获得不同角度比划的信息。</p>

<pre><code>
I = imread('~/Desktop/11.png')
J = rgb2gray(I);
K = imresize(J,[64,64])
gb=gabor_fn(64,64,1.2,theta,5,0,1)

</code></pre>

<p>这里的theta分别是pi/2, pi/4, pi和 3*pi/4 这个四个值，他们可以分别抓取中文字比划的那8个方向。
下面就是这个函数能产生的效果</p>

<p><img class="center" src="http://kkx.github.com/images/gabor.png" width="100" height="100" title="原图" >
<img class="center" src="http://kkx.github.com/images/gabor45.png" width="100" height="100" title="45º" >
<img class="center" src="http://kkx.github.com/images/gabor90.png" width="100" height="100" title="90º" >
<img class="center" src="http://kkx.github.com/images/gabor135.png" width="100" height="100" title="135º" >
<img class="center" src="http://kkx.github.com/images/gabor180.png" width="100" height="100" title="180º" ></p>

<p>冲上面的图就能看出这个过滤器的威力，过滤图片之后，我们可以对每个图片其中某个的区域的点阵进行统计，这样生成一个histogram，通过histogram可以有效的对每个字进行区分。</p>

<p>在我的试验中，我用了16个16<em>16大小的方格作为第一次层histogram数组，然后在它的上面又叠加了9个16</em>16的数组，总共是一个长(16+9)*4 = 100的数组。这么长的数组以后还的进行pca或者lda什么。实验在进行中。。。</p>
]]></content>
  </entry>
  
</feed>
