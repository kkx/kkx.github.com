<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: ocr | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/ocr/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2012-06-28T19:34:14-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[生成中文字图片]]></title>
    <link href="http://kkx.github.com/blog/2012/06/28/sheng-cheng-zhong-wen-zi-tu-pian/"/>
    <updated>2012-06-28T17:17:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/06/28/sheng-cheng-zhong-wen-zi-tu-pian</id>
    <content type="html"><![CDATA[<p>最近在做一个中文ocr，感觉是个不可完成任务。不管怎么样还是得试试。
首先要做的就是获得训练数据。中文的数据不比英文，量大而且没有地方下载，唯一能做的就是自己生成，在网上找了写函数自己改改就成了下面的脚本了</p>

<p>首先是一个中文字符集编码的生成函数</p>

<p>~~~~~~~~~</p>

<p>def generate_ch():
    #Function which generate all chinese gb2312 characters </p>

<pre><code>body = random.randint(0xA, 0xA)
tail = random.randint(0, 0xF)
chars = []

heads = range(0xB0, 0xF8)
bodys = range(0xA, 0xF+1)
tails = range(0, 0xF+1)
for head in heads:
    for body in bodys:
        for tail in tails:
            val = ( head &lt;&lt; 0x8 ) | (body &lt;&lt; 0x4 ) | tail
            #print "%x"%head,"%x"%body,"%x"%tail,"%x"%val
            str = "%x" % val
            if str.endswith("ff") or str.endswith("a0"):
                continue
            #special cases no characters with these codecs
            if str == "d7fa" or str == "d7fb" or str == "d7fc" or str == "d7fd" or str == "d7fe" or str == "d7ff" :
                continue
            chars.append(str.decode('hex').decode('gb2312'))
return chars
</code></pre>

<pre><code>

如果要生成中文字符图片，有很多办法，这里我用pygame打印每个字符

</code></pre>

<h1 id="coding-utf-8---">-<em>- coding: utf-8 -</em>-</h1>
<p>import pygame
 pygame.init()
font = pygame.font.Font(os.path.join(“fonts”, “simfang.ttf”), 64)
rtext = font.render(““.join(“b1a1”.decode(‘hex’).decode(‘gb2312’)), True, (0, 0, 0), (255, 255, 255))
pygame.image.save(rtext,”t.jpg” )</p>

<p>~~~~~~~~</p>

<p>然后中文字符图片就出来了
<img class="center" src="http://kkx.github.com/images/character.jpg" width="300" height="300" title="原图" ></p>

<p>要打印字符，需要至少一个ttf,也就是字体的文件。这里是用的simfang.ttf</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Gabor filters对中文字图片的特征提取]]></title>
    <link href="http://kkx.github.com/blog/2012/06/28/gabor-filtersdui-zhong-wen-zi-tu-pian-de-te-zheng-ti-qu/"/>
    <updated>2012-06-28T16:52:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/06/28/gabor-filtersdui-zhong-wen-zi-tu-pian-de-te-zheng-ti-qu</id>
    <content type="html"><![CDATA[<p>中文字ocr是个比较讨厌的事情，主要是因为中文字繁多，光简体字就有6000多个以上，虽然其中大部分简体字是不常用的。对于这么一个这么大的，种类繁多的数据集，在分类的时候真心头疼，看了很多paper后，发现现在的state of the art是基于gabor filters，通过这个过滤器，对一个中文字的不同角度的比划进行过滤，然后在每个subregion里的每个角度的比划进行统计之后生成一个histogram，和bag of words一样，histogram可以用来对每个字进行分类或者用来寻找最近邻。</p>

<p>gabor filters 是个复杂的东西，网上的方程各种各样的，数学不好的小白就很难明白。在网上找了好多现成的函数之后，最后找到一个能用的matlab函数:</p>

<pre><code>
function gb=gabor_fn(rows,cols,sigma,theta,lambda,psi,gamma)
     
    sigma_x = sigma;
    sigma_y = sigma/gamma;
     
    % Bounding box
    nstds = 3;
    xmax = max(abs(nstds*sigma_x*cos(theta)),abs(nstds*sigma_y*sin(theta)));
    xmax = ceil(max(1,xmax));
    ymax = max(abs(nstds*sigma_x*sin(theta)),abs(nstds*sigma_y*cos(theta)));
    ymax = ceil(max(1,ymax));
    xmin = -xmax; ymin = -ymax;

    [x,y] = meshgrid(xmin:xmax,ymin:ymax);
    %[x,y] = meshgrid(-rows/2:rows/2-1,-cols/2:cols/2-1);

    % Rotation 
    x_theta=x*cos(theta)+y*sin(theta);
    y_theta=-x*sin(theta)+y*cos(theta);
    gb= exp(-.5*(x_theta.^2/sigma_x^2+y_theta.^2/sigma_y^2)).*cos(2*pi/lambda*x_theta+psi);
</code></pre>

<p>rows和cols是图片的大小, sigma是gauss的那个sigma，越大的话周围的pixels影响越大,theta要提取的比划的角度，lambda这个貌似是什么的长度(不明白啊),psi这里没用,gamma生成y_sigma。</p>

<p>通过改变theta这个角度值可以获得不同角度比划的信息。</p>

<pre><code>
I = imread('~/Desktop/11.png')
J = rgb2gray(I);
K = imresize(J,[64,64])
gb=gabor_fn(64,64,1.2,theta,5,0,1)

</code></pre>

<p>这里的theta分别是pi/2, pi/4, pi和 3*pi/4 这个四个值，他们可以分别抓取中文字比划的那8个方向。
下面就是这个函数能产生的效果</p>

<p><img class="center" src="http://kkx.github.com/images/gabor.png" width="100" height="100" title="原图" >
<img class="center" src="http://kkx.github.com/images/gabor45.png" width="100" height="100" title="45º" >
<img class="center" src="http://kkx.github.com/images/gabor90.png" width="100" height="100" title="90º" >
<img class="center" src="http://kkx.github.com/images/gabor135.png" width="100" height="100" title="135º" >
<img class="center" src="http://kkx.github.com/images/gabor180.png" width="100" height="100" title="180º" ></p>

<p>冲上面的图就能看出这个过滤器的威力，过滤图片之后，我们可以对每个图片其中某个的区域的点阵进行统计，这样生成一个histogram，通过histogram可以有效的对每个字进行区分。</p>

<p>在我的试验中，我用了16个16<em>16大小的方格作为第一次层histogram数组，然后在它的上面又叠加了9个16</em>16的数组，总共是一个长(16+9)*4 = 100的数组。这么长的数组以后还的进行pca或者lda什么。实验在进行中。。。</p>
]]></content>
  </entry>
  
</feed>
