<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: google | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/google/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2012-07-08T04:23:52-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[tesseract-ocr 中文字符识别速度慢的问题]]></title>
    <link href="http://kkx.github.com/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti/"/>
    <updated>2012-07-08T04:01:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti</id>
    <content type="html"><![CDATA[<p>在做这个ocr项目的时候，首先想到的是用一些开源的项目，通过比较，只有google对中文的支持度算是有好的，通过测试，它的识别度也还是可以。但是会面临到好几个问题,可以察觉到2个主要问题</p>

<ul>
  <li>中文字相比英语字符，识别速度低了好几倍</li>
  <li>中文字符有时候会整行出现大面积的错误，而且非常离谱，甚至有乱码的出现。</li>
</ul>

<p>通过这篇<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/35248.pdf">Adapting the Tesseract Open Source OCR Engine for Multilingual OCR</a>的论文里看，这个项目的框架为了能让各种不同语言的文字进行识别，把中文文字识别以一行句子为切分单位，以文字部首为最基本的识别单位(使用了connected components analysis切分所有不连在一起的部首(也可以用项目的图形debug界面http://code.google.com/p/tesseract-ocr/wiki/ViewerDebugging，能看到中文字是给切分开的)。识别之后，通过best first search对中文字符进行匹配。结果就是，文字识别速度较慢，大部分计算的开销用在了匹配的时候。由于用到了best-first 不能保证全局最优解，往往图像稍微有一些噪音就会会出现大面积的错误的情况，一开始的几个字也许和原本字符相似度还可以，句子后半部的字符就不是了。</p>

<p>这个方法完全是按英语的识别方法，通过以英语单词为基本匹配单位，匹配每个字母成为最有可能的单词。这样的一个方法可以让语言的semantic运用到识别了，大大提高的每个单词的识别度。但是这个方法的本身是特别适合语英语，每个单词的长度不是很大，单词和单词之间有一个明显的空格，可以很好的切分。 但是运用到中文字符识别的时候确出现了以下几个问题:</p>

<ul>
  <li>以google那些人的看法，中文字符之间空隙是很小的，甚至很多时候是字和字之间是连在一起。其实印刷体的中文字符之间的空隙还是很明显的。</li>
  <li>中文句子长度一般都很长，往往匹配的时候需要很多开销</li>
  <li>中文识别的时候，我试过，就算把文本字符之间的空隙分的很大，还是会把一个字的之间的部首分开，在匹配的时候不会记住字符之间的空隙。</li>
</ul>

]]></content>
  </entry>
  
</feed>
