<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 机器学习 | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/机器学习/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2013-06-09T02:36:52-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[关于Knn和Random Forests的感觉]]></title>
    <link href="http://kkx.github.com/blog/2012/12/02/guan-yu-knnhe-random-forestsde-gan-jue/"/>
    <updated>2012-12-02T21:25:00-08:00</updated>
    <id>http://kkx.github.com/blog/2012/12/02/guan-yu-knnhe-random-forestsde-gan-jue</id>
    <content type="html"><![CDATA[<p>家里没网，只好扯扯淡了。</p>

<p>虽然一个是无监督学习，一个是监督学习，但是我感觉两个算法是近亲。 knn的就是找最近友邻，通过数据点和数据点之间的位置关系，而随机森林的话，是通过信息熵把类似的数据都放在一个叶子上。 其实这个感觉以前看到在集智俱乐部(其实就去过一次)有一面之缘的大牛的豆瓣<a href="http://www.douban.com/note/212245564/">博客</a>上写到的，但是当时没有领悟到里面的缘由。毕业论文用到的分类算法就这两个，现在就觉着他们是近亲。</p>

<p>两个算法各有优缺点，先说knn吧，不需要训练，只要有个好点的数据结构储存数据让查询的效率高一些就好(以前的博客有写)，找到的总是相似度最高的友邻。这样的好处就是不”吃亏”啦，没有loss。唯一要确定的的参数也就是取多少友邻介一个，这种naive算法给像我这种初学机器学习吐阳图性破的人用再好不过了。不过在测试的时候的速度是knn的软肋。</p>

<p>那随机森林呢，你要随个鸡鸡森林的话，设置起来就难啦，首先森林是基于树木的，你要学会ID3, C5(不是炸弹)决策树什么的，信息熵，gini啊什么的,各种数据分割的criteria，麻烦的很，要设置参数也多，什么多少层熟，一个森林多少树等等。不过参数多也有好处，就是可适性高。随机森林的话，有点在于速度超级快，从root到leaf的这段过程就是一个knn最近友邻的查找过程。一般在每个节点就是一个很简单的比较，所以速度超级快。结果会有loss，不一定是最相似的友邻。</p>

<p>介绍完两个算法的脾气后，得说说我最近几天和他们相处的感觉。knn适用于数据少的时候。RF用于数据多的时候。首先速度上来说knn就是这样的，另外就是，数据少的时候，友邻质量对你来说特别重要，你都没几个朋友了还都是帮土匪的话那你也就完了。RF作为一种boosting方法，本身就是个statistic概念很强的东西，通过对每个树加随机因子(数据上的或者特征上的随机，也有别的随机方式，譬如增加噪音什么的)让每棵看着差不多，用起来不一样:decorrelation。 这样通过增加variance的方法让结果更好点。但是如果数据少的话，就会适得其反啦。类似的，数据多的时候，knn拿到的优质友邻都很类似，那其实结果就是拿到了更少的友邻，换个角度看的话，你在射交网络天天关注的就几个人，这几个人还都是吃喝拉撒在一个地方的近亲，那他们给的信息大部分都差不多的，这时候你需要weak tie的友邻啦(好吧，我又扯到射交网络了，sorry)。而这个knn的问题rf就能弥补。这里你要说了，你不是说友邻质量很重要么？对，那是在数据少的时候，因为少，从概率上讲友邻的他们之间相似程度不会很高。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[FAST APPROXIMATE NEAREST NEIGHBORS(FLANN)]]></title>
    <link href="http://kkx.github.com/blog/2012/09/24/fast-approximate-nearest-neighbors-flann/"/>
    <updated>2012-09-24T20:04:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/24/fast-approximate-nearest-neighbors-flann</id>
    <content type="html"><![CDATA[<p>最近需要使用最近友邻对高纬度的数据进行查询。在实验的过程中发现相对于高纬度的数据，kd树的查找速度其实没有线性查找来的快的，但是在大数据面前，线性查找的开销是很大的。我在网上找了一些相关资料，最后发现有个叫flann的库很不错，这个库可以让根据用户设置的查找准确性确定性和用户提供的数据自动生成一个合理高效的数据结构，让每次搜索的速度比线性查找快几个数量级，同时丢失一些准确度。程序上自动的对参数和数据结构的设置，让使用者不用担心该使用什么样的方法来解决问题。</p>

<p>网上有各类优化的最近友邻查找方法的论文，很多时候针对每个问题，用的方法不一样能产生不同的优化。通过论文实验报告，有2个方法是在比较不错的:</p>

<ul>
  <li>Randomized kd-tree algorithm: 不同于普通的kd-tree，这个方法是通过生成一组(大于等于1)的kd树来储存数据。该方法是通过在最之前的D次随机数据分割从而获得方差最大的分割(D=5)。当搜索的时候所有树会根据和数据的距离进行排列，距离近的树先被排查，只会查找固定数目的leaf。</li>
  <li>Hierarchical k-means tree algorithm: 这个算法就是每次把数据进行k组分割(k-means) 每分割完的小组继续进行k组分割，直至每组数据小于K。这里随机部分是:首先查询数据会在树里过一遍，到达叶子的时候会生成一个优先队列，在队列里，查询数据到达最有叶子里的路径里的所有节点里没有访问过的branch, 队列的排列方式根据他们的中心位置和查询数据的距离从小到达排列。每次会从优先队列里提取一个节点从那里一直到树叶进行查找最近友邻，并同时在优先队列里插入新的未访问branch。查找的次数根据用户的提供的准确率百分比。</li>
</ul>

<p>最有参数的自动选择没有怎么整明白(刻意关系)，貌似是使用10%的数据做一个检测，参数的选择会在固定的几个之中进行选择。用户使用的时候往往需要提供一些使用场景的参数，比如: 查找速度，建表速度，内存使用速度，更具这几个参数，他会自动给你找到最优的方法和方法参数。只提供使用场景的这些参数更直观的让不是怎么了解算法本身的人能够用最合适的算法做最合适的事了。具体cost计算方程在论文的程式1中。</p>

<p>使用的感脚良好，很简单，速度也靠谱。有python matlab 等的binding，opencv里还自带了这个库！在mac上安装matlab上安装的时候碰到几个小问题。安装步骤可以参考<a href="http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html">http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html</a>。当中需要装一个patch让matlab里的mex可以能编译。地址在<a href="http://www.mathworks.es/support/solutions/en/data/1-FR6LXJ/">这里</a>装完之后更新下配置文件就能在matlab上编译c++文件了！</p>

<p>这个库的使用也是很容易的:    </p>

<pre><code>[index, search_params, speedup] = flann_build_index(mat_train_data(:,:), struct('algorithm','autotuned', 'target_precision',0.95, 'build_weight',0.01, 'memory_weight',0))
%这里表示无所谓内存开销，建立索引开销重视一般。
%搜索        
[result, ndists] = flann_search(index, mat_train_data(:,1), 5, search_params);
%保存
flann_save_index(index,'train.idx')
</code></pre>

<p>不过还是有很蛋疼的地方，比如，有时候你得save一下再重新load之后才能搜索, 还有就是里面数据和matlab的数据表现形式不一样啊，让我废了好久才弄明白，一般数据分析总是会把数据一行行保存，每个row是一条数据。但是这里他是反人类的反过来了！使用的时候要非常注意。</p>

<p>参考:</p>

<ul>
  <li>1 FAST APPROXIMATE NEAREST NEIGHBORS WITH AUTOMATIC ALGORITHM CONFIGURATION</li>
  <li>2 http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html</li>
  <li>3 http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN</li>
</ul>
]]></content>
  </entry>
  
</feed>
