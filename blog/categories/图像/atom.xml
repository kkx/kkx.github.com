<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 图像 | 名字]]></title>
  <link href="http://kkx.github.com/blog/categories/图像/atom.xml" rel="self"/>
  <link href="http://kkx.github.com/"/>
  <updated>2013-06-09T02:33:41-07:00</updated>
  <id>http://kkx.github.com/</id>
  <author>
    <name><![CDATA[小逸]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 4基于图片不变特征点]]></title>
    <link href="http://kkx.github.com/blog/2012/12/27/gao-du-xiang-si-tu-pian-jian-ce-4ji-yu-tu-pian-bu-bian-te-zheng-dian/"/>
    <updated>2012-12-27T13:44:00-08:00</updated>
    <id>http://kkx.github.com/blog/2012/12/27/gao-du-xiang-si-tu-pian-jian-ce-4ji-yu-tu-pian-bu-bian-te-zheng-dian</id>
    <content type="html"><![CDATA[<p>上面三篇高度相似图片检测没有讲到的一个方法就是基于图片特征点这个feature做图片相似检测。 图片特征点源于1999年Lowe 那篇”Object recognition from local scale-invariant features”
该文可以说是在图像识别形成了一前一后的格局，几年之后各个基于Lowe特征点的方法出来了，而原来基于颜色或者texture的方法就相对于naive了。以后的几年各种优化方法都出来了。
Lowe的方法是Scale-invariant feature transform 简称SIFT，从名字上来看这个方法对于图片的大小变化能起到很好忽略。这里有一点就是，这些特征点的主要目的就是
两张类似图片上的类似物体的特征点值是类似的，而不相似物体区域的特征点值是不一样的。在检测特征点优劣的时候主要就是检测一个repeatability。 Repeatability 就是只类似的物体特征点，
在经历不同的transformation之后的取回率，当然取回率高不一定好，还要看false positive，就类似信息检索。 transformation有很多种: 大小，旋转，颜色亮度，affine转变，occlusion什么的。
各种方法在针对不同的变换有不同的优化。</p>

<p>市面上有很多图片特征检测器和变种，下面是我用过的一些。
<img class="right" src="http://upload.wikimedia.org/wikipedia/commons/5/5f/Corner.png" width="300" height="300" title="角" ></p>

<ul>
  <li>Canny: 1986年出来的边检测器。现在还一直用，但不属于特征点范畴，更多用于基于boundary的方法。</li>
  <li>Harris corner detector: 边角检测器</li>
  <li>SIFT: 虽然这么叫，其实是通过高斯差获取scale space的最大最小值的位置。</li>
  <li>SURF: 类似SIFT，速度更快点。</li>
  <li>Harris-Affine 和 Hessian Affine: Mikolajczyk 04年的state of the art 方法。</li>
</ul>

<p>检测器检测出来的特征点要去’形容’它，把他转变成特征向量。最长用的就是SIFT descriptor，不局限于sift检测器，上述方法都可以用它，只要知道
特征点的位置，大小。一般检测特征点，会在不同的scale space上，每个scale space出来的特征点大小会不一样。SIFT特征向量是一个维度128的向量，再加上一个
特征区域的角度。 如下图所示，中心点周围的有4个16<em>16的格子，每个格子里有16个4</em>4的格子，会计算每个小格子的方向，划分成8个方向，每个4x4的小个子会成成一个长度为8
的向量，那总共有16个这样4*4大小的格子，所以最后会合成一个维度为128的向量。
<img class="center" src="http://kkx.github.com/images/keypointdescriptor1.JPG" width="600" height="600" title="SIFT" ></p>

<p>通过获得sift向量。一张图片能很好的给压缩成几k的向量矩阵。在检测相似图片的时候，对两张图片的sift向量逐一比对，如果距离在特定阀值之下，就能确定是相似的图片。
但是这样的话计算量是很大的， 因为通常一张1200*960的图片会有700-2000的sift特征点。那如果我的数据库里有1000张图片要比对的话，那就是很大个开销，
而且，sift向量逐一比对的时候，需要计算每对之间的距离。所以很多时候，会通过bag of words 把sift向量分类成x个cluster (x小于sift向量数), 然后把一张图片通过
它在每个cluster所拥有的sift向量数目来形容。这样一张图片就变成了一个长度为x的直方图。这样比对起来就会快了。虽然这样也会有很多信息丢失。但在实践中，这招非常有用。</p>

<p>基于特征点的方法的有点是对于旋转，affine上的变换会比前几个方法容忍度更高，在尺寸，亮度上的容忍也非常棒, 而且如果出现物体的遮盖occlusion，有时候也能检测出来。就是可能速度上没有前者的快。
这个特征点的方法不错，不过有一个致命的缺点就是所谓的semantic gap，往往你有很多你有很多sift的cluster，但是他们的空间信息都是丢失掉的。并且，一个sift和另一个sift之间的相对
距离也不会给利用到。会出现false positive，就是图片和另外一个外形上外向不相似的图片的相似度很高。</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[马赛克图片拼接]]></title>
    <link href="http://kkx.github.com/blog/2012/09/16/ma-sai-ke-tu-pian-pin-jie/"/>
    <updated>2012-09-16T13:21:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/16/ma-sai-ke-tu-pian-pin-jie</id>
    <content type="html"><![CDATA[<p>以前就想做一个马赛克合成的图片了，最近不务正业，就写了一个
方法很简单:</p>

<ul>
  <li>抓取一组图片的并缩放到统一大小</li>
  <li>计算每张图片的rgb值的中值</li>
  <li>所有的图片中值生成kd-tree</li>
  <li>加载目标图片，并缩放到想要的结果图片的大小，然后创建结果图片(np.zeros)</li>
  <li>每一小格结果图片的大小和抓取的小图大小一样,计算目标图片每小格的rgb 中值</li>
  <li>查找最想进的小图，并用小图的rgb值填充到结果图片里</li>
  <li>生成结果图片</li>
</ul>

<p>马赛克图片生成代码:</p>

<pre><code>import Image
import os
import numpy as np
from scipy import spatial

#calculate a mean rgb of an image
def meanRGB(img):
    rgb = np.array(img.getdata())
        mean_r = np.mean(rgb[:,0])
        mean_g = np.mean(rgb[:,1]) 
        mean_b = np.mean(rgb[:,2])
        return [mean_r, mean_g, mean_b]

if __name__ == "__main__":
    #grid size
    mosaic_img_size = (20, 20)
    #size of target image to generated
    target_img_size = (1000, 1000)
    target_img = Image.open("./VanGogh-starry_night_ballance1.jpg")
    target_img = target_img.resize(target_img_size)
    #grid images directory
    directory = "./icons/"
    listing = os.listdir(directory)
    img_list = []
    color_list = []
    #load images and calculate mean rgb
    for infile in listing:
        if infile.endswith("jpg"):
            img = Image.open(directory+infile)
            img = img.resize(mosaic_img_size, Image.ANTIALIAS)
            img_list.append(img)
            color_list.append(meanRGB(img))

    #create kdt for future pnn query
    data = np.array(color_list)
    tree = spatial.KDTree(data, leafsize=5)

    count_x = 0
    count_y = 0
    new_image = np.zeros((target_img_size[0],target_img_size[1],3), dtype=np.uint8)
    target_img_data = np.reshape(np.array(target_img.getdata()), (target_img_size[0],target_img_size[1],3))
    length = mosaic_img_size[0]*mosaic_img_size[1]

    #calculate mean rgb of each grid in the target image, search a nearest neighbor and fill the new image with
    #the rgb values of the grid image
    while count_x+mosaic_img_size[0] &lt; target_img_size[0]:
        count_y = 0
        while count_y+mosaic_img_size[1] &lt; target_img_size[1]:
            mean_r = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],0])
            mean_g = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],1])
            mean_b = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],2])
            img_index = tree.query(np.array([mean_r,mean_g,mean_b]))[1]
            new_image[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],:] = np.reshape(np.array(img_list[img_index].getdata()), (mosaic_img_size[0],mosaic_img_size[1],3))

            count_y += mosaic_img_size[1]
        count_x += mosaic_img_size[0]
    
    #image created
    im = Image.fromarray(new_image)
    im.show()
    im.save("./1VanGogh-starry_night_ballance1.jpg")
</code></pre>

<p>里面有些小bug，但不影响使用，譬如最右最下有一圈黑色的，是代码中某个逻辑没写好，没有填充进去，但是不管它了- -(貌似把小于号改成小于等于就可以了)</p>

<p>原图:
<img class="center" src="http://kkx.github.com/images/VanGogh-starry_night_ballance1.jpg" width="600" height="600" title="原图" >
效果:
<img class="center" src="http://kkx.github.com/images/1VanGogh-starry_night_ballance1.jpg" width="600" height="600" title="效果" >
768的向量空间长度，图片量一定要大一些，要不然的结果会是，颜色很单一，拼出来的马赛克效果不好。自己的友邻太少了，可能结果会不理想，所以这里我抓了豆瓣活动上的豆友，570多位。。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 3 Perceptual Hash(phash) 图片指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen/"/>
    <updated>2012-09-10T17:03:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen</id>
    <content type="html"><![CDATA[<p>最后一个要介绍的图片指纹生成方法: pHash, 这里是pHash算法简要:</p>

<ul>
  <li>图片指纹灰值化</li>
  <li>图片缩放到32x32</li>
  <li>计算机视觉每个图片的Discrete Cosine Transform (DCT, type II)。</li>
  <li>只保留最左上部的8x8的DCT系数(有32x32大)，这样保存了频率最低的那部分(图片信息的大部分)</li>
  <li>计算中值</li>
  <li>生成64维2维码，0表示系数小于中值，反之为1.</li>
</ul>

<p>说到DCT，其实就想傅里叶转换一样的，把信号转换成很多不同频率和振幅的正玄曲线相加的结果。但是DCT只是用cosine函数。这样的话图片高频率部分会给擦去，只保留低频率部分。因为在给图片操作的时候，往往低频率的DCT系数会给保留，而且大部分图片的信息会给保留在这些低频率DCT系数里。(JPEG压缩也用这个方法来对图片进行压缩)。
DCT 会生成8x8的系数表，那最左上方的表示最低频率的元素，也是最重要的，越往右下的表示相对频率稍高的元素(好多信号出来的东西啊，头好大)
反正到最后能生成一个图片指纹，从别的作者的实验结果来看，效果是相对较好的。在这个网站上<a href="http://phash.org">phash.org</a>能下到开源的代码，在网站的demo上可以可以试试看计算图片的相似性，通过测试，只有DCT比较靠谱，另外两个的结果非常糟糕。相同的，这个算法的信息量为2^64，冲撞率应该不高，吧？</p>

<p><a href="http://phash.org">phash.org</a>网站上是用c/c++写的代码，没有python的binding，在<a href="https://github.com/polachok/py-phash/">https://github.com/polachok/py-phash/</a>可以找到一个简陋的python binding，里面有我们需要的DCT的计算方法，调用一个函数就能直接获得指纹数值。测试了几下，效果还不错。不过还是需要很大量的图片用语测试false positive的概率。
在github上能看到作者写的使用方法说明，我这里具体的代码如下:</p>

<pre><code>
import pHash
import sys

if __name__ == "__main__":
    hash1 = pHash.imagehash(sys.argv[1])
    hash2 = pHash.imagehash(sys.argv[2])
    print 'Hamming distance: %d (%08x / %08x)' % ( pHash.hamming_distance( hash1, hash2 ), hash1, hash2 )

</code></pre>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 2图片缩小指纹]]></title>
    <link href="http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen/"/>
    <updated>2012-09-09T00:09:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen</id>
    <content type="html"><![CDATA[<p>图片缩小之后其实保留个部分图片的信息，可以用来当做图片的指纹，在查找相关资料的时候发现了一个很不错的<a href="http://hackerlabs.org/blog/2012/07/30/organizing-photos-with-duplicate-and-similarity-checking/">博客</a>
在这个博客里，几乎所有的和图片相似性计算的算法都在了(除了基于语义的)。超级不错。里面的很多算法的实现是用pearl的，而本身抱着学习的态度我决定重新用python对其中的某些算法实现一遍。</p>

<p>这里是图片缩小指纹的算法:</p>

<ul>
  <li>缩放到160x160的图片</li>
  <li>灰值化</li>
  <li>模糊图片，把噪音去除掉</li>
  <li>equalize，让图片反差更大</li>
  <li>计算图片平均值</li>
  <li>缩放到16x16的图，并且降到2维，更具图片pixel值:大于平均值的为1，反之为0</li>
</ul>

<p>这样就生成了图片的指纹，信息总量有2^256, 哈希冲撞应该不大。
计算的图片相似度的时候直接使用xor。公式可以是这样: 1 - xor(p1,p2)/256</p>

<p>具体python实现代码如下:</p>

<pre><code>from PIL import Image
import sys
import ImageFilter
import ImageOps 
import numpy as np

def normalize(img, size = (160, 160)):
    return ImageOps.equalize(img.resize(size).convert('L').filter(ImageFilter.BLUR))

def calculate_fingerprint(img):
    img = normalize(img)
    mean_value = np.mean(np.array(img.getdata()))
    img = img.resize((16,16))
    return np.array(img.getdata())&gt;mean_value

def calculate_simility_by_path(img_path1, img_path2):
    img1 = Image.open(img_path1)
    img2 = Image.open(img_path2)
    fingerprint1 = calculate_fingerprint(img1) 
    fingerprint2 = calculate_fingerprint(img2) 
    return 1 - (0.0+np.sum(np.logical_xor(fingerprint1, fingerprint2)))/fingerprint1.size

if __name__ == "__main__":
    print calculate_simility_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>更好的implementation可以在这个<a href="http://www.ostertag.name/HowTo/findimagedupes.pl">链接</a>找到，虽然做法上不一样，但是原理应该都差不多。在代码的注释里，能看到这个类算法的优点和缺点，如果图片相似度较高的话，识别起来没什么问题。最大的缺点是，很多false positive，如果图片是一张大海的话 会出现一下情况</p>

<pre><code>1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
</code></pre>

<p>作者还说到对于大规模数据的话，性能会降低很多。但是貌似这个能用bloomfilter解决，现在先不管他</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[高度相似图片检测: 1颜色直方图差]]></title>
    <link href="http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai/"/>
    <updated>2012-09-08T15:30:00-07:00</updated>
    <id>http://kkx.github.com/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai</id>
    <content type="html"><![CDATA[<p>对于颜色直方图差的比对能很好的检测出图片之间的色差变化，那对于两张环境，光线，角度长不错的图片，能很好的检测出他们的相似度。
这是一个很老的方法，简单粗暴，非常使用。原理在这篇<a href="http://blog.csdn.net/lanphaday/article/details/2325027">博客</a>里很清楚的讲解了。 算法很简单:</p>

<ul>
  <li>把两张图片缩放到统一的大小，这里可以是256x256， 并且统一成rgb模式。</li>
  <li>统计每张图片的每个颜色的出现次数。那这里因为是rgb模式，有3*256=768 种颜色，生成直方图</li>
  <li>对两张图片的直方图进行比对。如果统计结果一致的话，相似度+1</li>
</ul>

<p>这里有个问题就是对空间信息的丢失，原作者通过把图片分割成16个小方格来解决这个问题。(split_image函数)</p>

<p>具体代码如下:</p>

<pre><code>from PIL import Image
import sys
def normalize(img, size = (256, 256)):
    return img.resize(size).convert('RGB')

def sim_hist(hist1, hist2):
    assert len(hist1) == len(hist2)
    return sum(1 - (0 if hist1 == hist2 else float(abs(hist1 - hist2))/max(hist1, hist2)) for hist1, hist2 in zip(hist1, hist2))/len(hist1)


def calc_similar_by_path(path_img1, path_img2):
    li = normalize(Image.open(path_img1))
    ri = normalize(Image.open(path_img2))
    return sum(sim_hist(l.histogram(), r.histogram()) for l, r in zip(split_image(li), split_image(ri))) / 16.0



def split_image(img, part_size = (64, 64)):
    w, h = img.size
    pw, ph = part_size
    assert w % pw == h % ph == 0
    return [img.crop((i, j, i+pw, j+ph)).copy() \
        for i in xrange(0, w, pw) \
        for j in xrange(0, h, ph)]

if __name__ == "__main__":
    print calc_similar_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>这里的fingerprint就是每个图片的直方图。 这个指纹的信息量的话应该挺大的(至少有2^768) 理论上来讲应该false positive的可能性很小，但是这个结论应该是错的，因为在图片里很多颜色是出现的概率是很小的，大部分颜色都集中于某些值中。</p>

<p>代码和原作者的差不多，实验结果是不错的，很简单，但是这个算法有个致命的问题就是对颜色的过分依赖。对于图片角度的变化的容忍度挺高，但是如果颜色或者图片光线变化稍大就不能很好的检测出相似度了，会把相似的图片判断为不同的。最简单的方法就是弄个灰度图，这方法基于rgb的，就无解了。还有个小问题就是，false positive，我觉得如果两张白底黑字的文字图片，不管字是多么不一样，但是他们的相似度应该很高(在有的应用上，这个结果是很希望的)</p>

]]></content>
  </entry>
  
</feed>
