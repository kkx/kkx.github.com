
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>名字</title>
  <meta name="author" content="小逸">

  
  <meta name="description" content="上面三篇高度相似图片检测没有讲到的一个方法就是基于图片特征点这个feature做图片相似检测。 图片特征点源于1999年Lowe 那篇”Object recognition from local scale-invariant features”
该文可以说是在图像识别形成了一前一后的格局， &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://kkx.github.com">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="/javascripts/ender.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/atom.xml" rel="alternate" title="名字" type="application/atom+xml">
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-31462157-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<script type="text/javascript"
        src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
      MathJax.InputJax.TeX.prefilterHooks.Add(function (data) {
    data.math = data.math.replace(/^\s*<!\[CDATA\[\s*((?:\n|.)*)\s*\]\]>\s*$/m,"$1");
});
});
</script>

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">名字</a></h1>
  
    <h2>没想好</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:kkx.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/about/">About me</a></li>
  <li><a href="/plan/">Plans</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">
  
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/27/gao-du-xiang-si-tu-pian-jian-ce-4ji-yu-tu-pian-bu-bian-te-zheng-dian/">高度相似图片检测: 4基于图片不变特征点</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-27T13:44:00-08:00" pubdate data-updated="true">Dec 27<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/12/27/gao-du-xiang-si-tu-pian-jian-ce-4ji-yu-tu-pian-bu-bian-te-zheng-dian/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>上面三篇高度相似图片检测没有讲到的一个方法就是基于图片特征点这个feature做图片相似检测。 图片特征点源于1999年Lowe 那篇”Object recognition from local scale-invariant features”
该文可以说是在图像识别形成了一前一后的格局，几年之后各个基于Lowe特征点的方法出来了，而原来基于颜色或者texture的方法就相对于naive了。以后的几年各种优化方法都出来了。
Lowe的方法是Scale-invariant feature transform 简称SIFT，从名字上来看这个方法对于图片的大小变化能起到很好忽略。这里有一点就是，这些特征点的主要目的就是
两张类似图片上的类似物体的特征点值是类似的，而不相似物体区域的特征点值是不一样的。在检测特征点优劣的时候主要就是检测一个repeatability。 Repeatability 就是只类似的物体特征点，
在经历不同的transformation之后的取回率，当然取回率高不一定好，还要看false positive，就类似信息检索。 transformation有很多种: 大小，旋转，颜色亮度，affine转变，occlusion什么的。
各种方法在针对不同的变换有不同的优化。</p>

<p>市面上有很多图片特征检测器和变种，下面是我用过的一些。
<img class="right" src="http://upload.wikimedia.org/wikipedia/commons/5/5f/Corner.png" width="300" height="300" title="角" /></p>

<ul>
  <li>Canny: 1986年出来的边检测器。现在还一直用，但不属于特征点范畴，更多用于基于boundary的方法。</li>
  <li>Harris corner detector: 边角检测器</li>
  <li>SIFT: 虽然这么叫，其实是通过高斯差获取scale space的最大最小值的位置。</li>
  <li>SURF: 类似SIFT，速度更快点。</li>
  <li>Harris-Affine 和 Hessian Affine: Mikolajczyk 04年的state of the art 方法。</li>
</ul>

<p>检测器检测出来的特征点要去’形容’它，把他转变成特征向量。最长用的就是SIFT descriptor，不局限于sift检测器，上述方法都可以用它，只要知道
特征点的位置，大小。一般检测特征点，会在不同的scale space上，每个scale space出来的特征点大小会不一样。SIFT特征向量是一个维度128的向量，再加上一个
特征区域的角度。 如下图所示，中心点周围的有4个16<em>16的格子，每个格子里有16个4</em>4的格子，会计算每个小格子的方向，划分成8个方向，每个4x4的小个子会成成一个长度为8
的向量，那总共有16个这样4*4大小的格子，所以最后会合成一个维度为128的向量。
<img class="center" src="http://localhost:4000/images/keypointdescriptor1.JPG" width="600" height="600" title="SIFT" /></p>

<p>通过获得sift向量。一张图片能很好的给压缩成几k的向量矩阵。在检测相似图片的时候，对两张图片的sift向量逐一比对，如果距离在特定阀值之下，就能确定是相似的图片。
但是这样的话计算量是很大的， 因为通常一张1200*960的图片会有700-2000的sift特征点。那如果我的数据库里有1000张图片要比对的话，那就是很大个开销，
而且，sift向量逐一比对的时候，需要计算每对之间的距离。所以很多时候，会通过bag of words 把sift向量分类成x个cluster (x小于sift向量数), 然后把一张图片通过
它在每个cluster所拥有的sift向量数目来形容。这样一张图片就变成了一个长度为x的直方图。这样比对起来就会快了。虽然这样也会有很多信息丢失。但在实践中，这招非常有用。</p>

<p>基于特征点的方法的有点是对于旋转，affine上的变换会比前几个方法容忍度更高，在尺寸，亮度上的容忍也非常棒, 而且如果出现物体的遮盖occlusion，有时候也能检测出来。就是可能速度上没有前者的快。
这个特征点的方法不错，不过有一个致命的缺点就是所谓的semantic gap，往往你有很多你有很多sift的cluster，但是他们的空间信息都是丢失掉的。并且，一个sift和另一个sift之间的相对
距离也不会给利用到。会出现false positive，就是图片和另外一个外形上外向不相似的图片的相似度很高。</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/12/02/guan-yu-knnhe-random-forestsde-gan-jue/">关于Knn和Random Forests的感觉</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-12-02T21:25:00-08:00" pubdate data-updated="true">Dec 2<span>nd</span>, 2012</time>
        
         | <a href="/blog/2012/12/02/guan-yu-knnhe-random-forestsde-gan-jue/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>家里没网，只好扯扯淡了。</p>

<p>虽然一个是无监督学习，一个是监督学习，但是我感觉两个算法是近亲。 knn的就是找最近友邻，通过数据点和数据点之间的位置关系，而随机森林的话，是通过信息熵把类似的数据都放在一个叶子上。 其实这个感觉以前看到在集智俱乐部(其实就去过一次)有一面之缘的大牛的豆瓣<a href="http://www.douban.com/note/212245564/">博客</a>上写到的，但是当时没有领悟到里面的缘由。毕业论文用到的分类算法就这两个，现在就觉着他们是近亲。</p>

<p>两个算法各有优缺点，先说knn吧，不需要训练，只要有个好点的数据结构储存数据让查询的效率高一些就好(以前的博客有写)，找到的总是相似度最高的友邻。这样的好处就是不”吃亏”啦，没有loss。唯一要确定的的参数也就是取多少友邻介一个，这种naive算法给像我这种初学机器学习吐阳图性破的人用再好不过了。不过在测试的时候的速度是knn的软肋。</p>

<p>那随机森林呢，你要随个鸡鸡森林的话，设置起来就难啦，首先森林是基于树木的，你要学会ID3, C5(不是炸弹)决策树什么的，信息熵，gini啊什么的,各种数据分割的criteria，麻烦的很，要设置参数也多，什么多少层熟，一个森林多少树等等。不过参数多也有好处，就是可适性高。随机森林的话，有点在于速度超级快，从root到leaf的这段过程就是一个knn最近友邻的查找过程。一般在每个节点就是一个很简单的比较，所以速度超级快。结果会有loss，不一定是最相似的友邻。</p>

<p>介绍完两个算法的脾气后，得说说我最近几天和他们相处的感觉。knn适用于数据少的时候。RF用于数据多的时候。首先速度上来说knn就是这样的，另外就是，数据少的时候，友邻质量对你来说特别重要，你都没几个朋友了还都是帮土匪的话那你也就完了。RF作为一种boosting方法，本身就是个statistic概念很强的东西，通过对每个树加随机因子(数据上的或者特征上的随机，也有别的随机方式，譬如增加噪音什么的)让每棵看着差不多，用起来不一样:decorrelation。 这样通过增加variance的方法让结果更好点。但是如果数据少的话，就会适得其反啦。类似的，数据多的时候，knn拿到的优质友邻都很类似，那其实结果就是拿到了更少的友邻，换个角度看的话，你在射交网络天天关注的就几个人，这几个人还都是吃喝拉撒在一个地方的近亲，那他们给的信息大部分都差不多的，这时候你需要weak tie的友邻啦(好吧，我又扯到射交网络了，sorry)。而这个knn的问题rf就能弥补。这里你要说了，你不是说友邻质量很重要么？对，那是在数据少的时候，因为少，从概率上讲友邻的他们之间相似程度不会很高。</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/10/19/c-plus-plus-li-shi-yong-octave/">C++里使用octave</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-10-19T22:17:00-07:00" pubdate data-updated="true">Oct 19<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/10/19/c-plus-plus-li-shi-yong-octave/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>蛋疼无极限，想在c++ 里跑octave代码，找到了两个帖子，拼拼凑凑终于可以跑了！
<a href="https://mailman.cae.wisc.edu/pipermail/help-octave/2009-April/034429.html">帖子1</a> 还是邮件列表里的私藏。
照着上面的代码跑一下会发现没有装octave的头文件</p>

<pre><code>    Hi,

    I tried to find information on how to call an octave .m file from a  
    C++ program. There is some information around, but most of it is no  
    longer up-to-date, it was not trivial to get it working. In this post  
    I want to explain how to do it, maybe it is useful to somebody.

    I have a file called exampleOctaveFunction.m which looks like this:

    -------------------------------------
    function [resultScalar, resultString, resultMatrix] =  
    exampleOctaveFunction (inScalar, inString, inMatrix)

    resultScalar = (inScalar * pi);
    resultString = strcat ('Good morning Mr. ', inString);
    resultMatrix = (inMatrix + 1);

    endfunction
    -------------------------------------

    I have a file called how-to-call-octave.cpp which is this:

    -------------------------------------
    #include &lt;octave/oct.h&gt;
    #include &lt;octave/octave.h&gt;
    #include &lt;octave/parse.h&gt;
    #include &lt;octave/toplev.h&gt; /* do_octave_atexit */

    int main (const int argc, char ** argv)
    {
    const char * argvv [] = {"" /* name of program, not relevant */,  "--silent"};

        octave_main (2, (char **) argvv, true /* embedded */);

        octave_value_list functionArguments;

        functionArguments (0) = 2;
        functionArguments (1) = "D. Humble";

        Matrix inMatrix (2, 3);

        inMatrix (0, 0) = 10;
        inMatrix (0, 1) = 9;
        inMatrix (0, 2) = 8;
        inMatrix (1, 0) = 7;
        inMatrix (1, 1) = 6;
        functionArguments (2) = inMatrix;

        const octave_value_list result = feval ("exampleOctaveFunction",  
        functionArguments, 1);

        std::cout &lt;&lt; "resultScalar is " &lt;&lt; result (0).scalar_value () &lt;&lt; std::endl;
        std::cout &lt;&lt; "resultString is " &lt;&lt; result (1).string_value () &lt;&lt; std::endl;
        std::cout &lt;&lt; "resultMatrix is\n" &lt;&lt; result (2).matrix_value ();

        do_octave_atexit ();
        }
    -------------------------------------

    And a little readme file, called readme.sh, that explains how to  
    compile and run this simple example:

    -------------------------------------
    #! /bin/bash

    #
    # Make sure you have octave installed. On my system, Ubuntu 8.10, I installed
    # the packages octave3.0 and octave3.0-headers.
    #
    # To compile, type
    #
    make how-to-call-octave.o
    #
    # which, on my system, is equivalent to
    #
    #   g++ -c -o how-to-call-octave.o how-to-call-octave.cpp
    #
    # To link, type
    #
    g++ -L /usr/lib/octave-3.0.1/ -l octinterp -o how-to-call-octave  
    how-to-call-octave.o
    #
    # Adjust the directory in the -L directive according to the configuration of
    # your machine.
    #
    # To run, type
    #
    ./how-to-call-octave
    #
    # It should output something like
    #
    #   resultScalar is 6.28319
    #   resultString is Good morning Mr. D. Humble
    #   resultMatrix is
    #    11 10 9
    #    8 7 1
    #
    # Hope this is of help to somebody.
    #
    -------------------------------------

    You can also execute the readme.sh file and it will compile and run for you.
    Good luck, Arjan.

</code></pre>

<p>apt-get 之后发现还是跑不起来, 在<a href="http://octave.1599824.n4.nabble.com/call-octave-from-C-linker-error-td3987581.html">帖子2</a>里找到了解决方法</p>

<pre><code>I started out trying to run the octave example as a c++ file(i.e. no Qt) and received the same errors. The problem is you have to tell the linker where the libs are located. As root, run "ldconfig /path/to/octave/libs". This is not permanent because the next time ldconfig is run it will not find the octave libs. 

To make sure the cache gets updated every time ldconfig is run (Ubuntu is a Debian derivative so I'm assuming the procedure is the same) do this:


sudo -i
cd /etc/ld.so.conf.d
touch octave.conf
vi (your fav. editor) octave.conf 
add a line: /path/to/octavelibs
save file
run ldconfig
</code></pre>

<p>设置后就能跑啦。最近的博客越来越水了。我又要烂掉了。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/24/fast-approximate-nearest-neighbors-flann/">FAST APPROXIMATE NEAREST NEIGHBORS(FLANN)</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-24T20:04:00-07:00" pubdate data-updated="true">Sep 24<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/24/fast-approximate-nearest-neighbors-flann/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>最近需要使用最近友邻对高纬度的数据进行查询。在实验的过程中发现相对于高纬度的数据，kd树的查找速度其实没有线性查找来的快的，但是在大数据面前，线性查找的开销是很大的。我在网上找了一些相关资料，最后发现有个叫flann的库很不错，这个库可以让根据用户设置的查找准确性确定性和用户提供的数据自动生成一个合理高效的数据结构，让每次搜索的速度比线性查找快几个数量级，同时丢失一些准确度。程序上自动的对参数和数据结构的设置，让使用者不用担心该使用什么样的方法来解决问题。</p>

<p>网上有各类优化的最近友邻查找方法的论文，很多时候针对每个问题，用的方法不一样能产生不同的优化。通过论文实验报告，有2个方法是在比较不错的:</p>

<ul>
  <li>Randomized kd-tree algorithm: 不同于普通的kd-tree，这个方法是通过生成一组(大于等于1)的kd树来储存数据。该方法是通过在最之前的D次随机数据分割从而获得方差最大的分割(D=5)。当搜索的时候所有树会根据和数据的距离进行排列，距离近的树先被排查，只会查找固定数目的leaf。</li>
  <li>Hierarchical k-means tree algorithm: 这个算法就是每次把数据进行k组分割(k-means) 每分割完的小组继续进行k组分割，直至每组数据小于K。这里随机部分是:首先查询数据会在树里过一遍，到达叶子的时候会生成一个优先队列，在队列里，查询数据到达最有叶子里的路径里的所有节点里没有访问过的branch, 队列的排列方式根据他们的中心位置和查询数据的距离从小到达排列。每次会从优先队列里提取一个节点从那里一直到树叶进行查找最近友邻，并同时在优先队列里插入新的未访问branch。查找的次数根据用户的提供的准确率百分比。</li>
</ul>

<p>最有参数的自动选择没有怎么整明白(刻意关系)，貌似是使用10%的数据做一个检测，参数的选择会在固定的几个之中进行选择。用户使用的时候往往需要提供一些使用场景的参数，比如: 查找速度，建表速度，内存使用速度，更具这几个参数，他会自动给你找到最优的方法和方法参数。只提供使用场景的这些参数更直观的让不是怎么了解算法本身的人能够用最合适的算法做最合适的事了。具体cost计算方程在论文的程式1中。</p>

<p>使用的感脚良好，很简单，速度也靠谱。有python matlab 等的binding，opencv里还自带了这个库！在mac上安装matlab上安装的时候碰到几个小问题。安装步骤可以参考<a href="http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html">http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html</a>。当中需要装一个patch让matlab里的mex可以能编译。地址在<a href="http://www.mathworks.es/support/solutions/en/data/1-FR6LXJ/">这里</a>装完之后更新下配置文件就能在matlab上编译c++文件了！</p>

<p>这个库的使用也是很容易的:    </p>

<pre><code>[index, search_params, speedup] = flann_build_index(mat_train_data(:,:), struct('algorithm','autotuned', 'target_precision',0.95, 'build_weight',0.01, 'memory_weight',0))
%这里表示无所谓内存开销，建立索引开销重视一般。
%搜索        
[result, ndists] = flann_search(index, mat_train_data(:,1), 5, search_params);
%保存
flann_save_index(index,'train.idx')
</code></pre>

<p>不过还是有很蛋疼的地方，比如，有时候你得save一下再重新load之后才能搜索, 还有就是里面数据和matlab的数据表现形式不一样啊，让我废了好久才弄明白，一般数据分析总是会把数据一行行保存，每个row是一条数据。但是这里他是反人类的反过来了！使用的时候要非常注意。</p>

<p>参考:</p>

<ul>
  <li>1 FAST APPROXIMATE NEAREST NEIGHBORS WITH AUTOMATIC ALGORITHM CONFIGURATION</li>
  <li>2 http://www.cs.uky.edu/~jacobs/tips/flann_matlab.html</li>
  <li>3 http://www.cs.ubc.ca/~mariusm/index.php/FLANN/FLANN</li>
</ul>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/16/ma-sai-ke-tu-pian-pin-jie/">马赛克图片拼接</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-16T13:21:00-07:00" pubdate data-updated="true">Sep 16<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/16/ma-sai-ke-tu-pian-pin-jie/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>以前就想做一个马赛克合成的图片了，最近不务正业，就写了一个
方法很简单:</p>

<ul>
  <li>抓取一组图片的并缩放到统一大小</li>
  <li>计算每张图片的rgb值的中值</li>
  <li>所有的图片中值生成kd-tree</li>
  <li>加载目标图片，并缩放到想要的结果图片的大小，然后创建结果图片(np.zeros)</li>
  <li>每一小格结果图片的大小和抓取的小图大小一样,计算目标图片每小格的rgb 中值</li>
  <li>查找最想进的小图，并用小图的rgb值填充到结果图片里</li>
  <li>生成结果图片</li>
</ul>

<p>马赛克图片生成代码:</p>

<pre><code>import Image
import os
import numpy as np
from scipy import spatial

#calculate a mean rgb of an image
def meanRGB(img):
    rgb = np.array(img.getdata())
        mean_r = np.mean(rgb[:,0])
        mean_g = np.mean(rgb[:,1]) 
        mean_b = np.mean(rgb[:,2])
        return [mean_r, mean_g, mean_b]

if __name__ == "__main__":
    #grid size
    mosaic_img_size = (20, 20)
    #size of target image to generated
    target_img_size = (1000, 1000)
    target_img = Image.open("./VanGogh-starry_night_ballance1.jpg")
    target_img = target_img.resize(target_img_size)
    #grid images directory
    directory = "./icons/"
    listing = os.listdir(directory)
    img_list = []
    color_list = []
    #load images and calculate mean rgb
    for infile in listing:
        if infile.endswith("jpg"):
            img = Image.open(directory+infile)
            img = img.resize(mosaic_img_size, Image.ANTIALIAS)
            img_list.append(img)
            color_list.append(meanRGB(img))

    #create kdt for future pnn query
    data = np.array(color_list)
    tree = spatial.KDTree(data, leafsize=5)

    count_x = 0
    count_y = 0
    new_image = np.zeros((target_img_size[0],target_img_size[1],3), dtype=np.uint8)
    target_img_data = np.reshape(np.array(target_img.getdata()), (target_img_size[0],target_img_size[1],3))
    length = mosaic_img_size[0]*mosaic_img_size[1]

    #calculate mean rgb of each grid in the target image, search a nearest neighbor and fill the new image with
    #the rgb values of the grid image
    while count_x+mosaic_img_size[0] &lt; target_img_size[0]:
        count_y = 0
        while count_y+mosaic_img_size[1] &lt; target_img_size[1]:
            mean_r = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],0])
            mean_g = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],1])
            mean_b = np.mean(target_img_data[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],2])
            img_index = tree.query(np.array([mean_r,mean_g,mean_b]))[1]
            new_image[count_x:count_x+mosaic_img_size[0],count_y:count_y+mosaic_img_size[1],:] = np.reshape(np.array(img_list[img_index].getdata()), (mosaic_img_size[0],mosaic_img_size[1],3))

            count_y += mosaic_img_size[1]
        count_x += mosaic_img_size[0]
    
    #image created
    im = Image.fromarray(new_image)
    im.show()
    im.save("./1VanGogh-starry_night_ballance1.jpg")
</code></pre>

<p>里面有些小bug，但不影响使用，譬如最右最下有一圈黑色的，是代码中某个逻辑没写好，没有填充进去，但是不管它了- -(貌似把小于号改成小于等于就可以了)</p>

<p>原图:
<img class="center" src="http://kkx.github.com/images/VanGogh-starry_night_ballance1.jpg" width="600" height="600" title="原图" />
效果:
<img class="center" src="http://kkx.github.com/images/1VanGogh-starry_night_ballance1.jpg" width="600" height="600" title="效果" />
768的向量空间长度，图片量一定要大一些，要不然的结果会是，颜色很单一，拼出来的马赛克效果不好。自己的友邻太少了，可能结果会不理想，所以这里我抓了豆瓣活动上的豆友，570多位。。</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen/">高度相似图片检测: 3 Perceptual Hash(phash) 图片指纹</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-10T17:03:00-07:00" pubdate data-updated="true">Sep 10<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/10/gao-du-xiang-si-tu-pian-jian-ce-3perceptual-hash-phash-tu-pian-zhi-wen/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>最后一个要介绍的图片指纹生成方法: pHash, 这里是pHash算法简要:</p>

<ul>
  <li>图片指纹灰值化</li>
  <li>图片缩放到32x32</li>
  <li>计算机视觉每个图片的Discrete Cosine Transform (DCT, type II)。</li>
  <li>只保留最左上部的8x8的DCT系数(有32x32大)，这样保存了频率最低的那部分(图片信息的大部分)</li>
  <li>计算中值</li>
  <li>生成64维2维码，0表示系数小于中值，反之为1.</li>
</ul>

<p>说到DCT，其实就想傅里叶转换一样的，把信号转换成很多不同频率和振幅的正玄曲线相加的结果。但是DCT只是用cosine函数。这样的话图片高频率部分会给擦去，只保留低频率部分。因为在给图片操作的时候，往往低频率的DCT系数会给保留，而且大部分图片的信息会给保留在这些低频率DCT系数里。(JPEG压缩也用这个方法来对图片进行压缩)。
DCT 会生成8x8的系数表，那最左上方的表示最低频率的元素，也是最重要的，越往右下的表示相对频率稍高的元素(好多信号出来的东西啊，头好大)
反正到最后能生成一个图片指纹，从别的作者的实验结果来看，效果是相对较好的。在这个网站上<a href="http://phash.org">phash.org</a>能下到开源的代码，在网站的demo上可以可以试试看计算图片的相似性，通过测试，只有DCT比较靠谱，另外两个的结果非常糟糕。相同的，这个算法的信息量为2^64，冲撞率应该不高，吧？</p>

<p><a href="http://phash.org">phash.org</a>网站上是用c/c++写的代码，没有python的binding，在<a href="https://github.com/polachok/py-phash/">https://github.com/polachok/py-phash/</a>可以找到一个简陋的python binding，里面有我们需要的DCT的计算方法，调用一个函数就能直接获得指纹数值。测试了几下，效果还不错。不过还是需要很大量的图片用语测试false positive的概率。
在github上能看到作者写的使用方法说明，我这里具体的代码如下:</p>

<pre><code>
import pHash
import sys

if __name__ == "__main__":
    hash1 = pHash.imagehash(sys.argv[1])
    hash2 = pHash.imagehash(sys.argv[2])
    print 'Hamming distance: %d (%08x / %08x)' % ( pHash.hamming_distance( hash1, hash2 ), hash1, hash2 )

</code></pre>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen/">高度相似图片检测: 2图片缩小指纹</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-09T00:09:00-07:00" pubdate data-updated="true">Sep 9<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/09/gao-du-xiang-si-tu-pian-jian-ce-2tu-pian-suo-xiao-zhi-wen/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>图片缩小之后其实保留个部分图片的信息，可以用来当做图片的指纹，在查找相关资料的时候发现了一个很不错的<a href="http://hackerlabs.org/blog/2012/07/30/organizing-photos-with-duplicate-and-similarity-checking/">博客</a>
在这个博客里，几乎所有的和图片相似性计算的算法都在了(除了基于语义的)。超级不错。里面的很多算法的实现是用pearl的，而本身抱着学习的态度我决定重新用python对其中的某些算法实现一遍。</p>

<p>这里是图片缩小指纹的算法:</p>

<ul>
  <li>缩放到160x160的图片</li>
  <li>灰值化</li>
  <li>模糊图片，把噪音去除掉</li>
  <li>equalize，让图片反差更大</li>
  <li>计算图片平均值</li>
  <li>缩放到16x16的图，并且降到2维，更具图片pixel值:大于平均值的为1，反之为0</li>
</ul>

<p>这样就生成了图片的指纹，信息总量有2^256, 哈希冲撞应该不大。
计算的图片相似度的时候直接使用xor。公式可以是这样: 1 - xor(p1,p2)/256</p>

<p>具体python实现代码如下:</p>

<pre><code>from PIL import Image
import sys
import ImageFilter
import ImageOps 
import numpy as np

def normalize(img, size = (160, 160)):
    return ImageOps.equalize(img.resize(size).convert('L').filter(ImageFilter.BLUR))

def calculate_fingerprint(img):
    img = normalize(img)
    mean_value = np.mean(np.array(img.getdata()))
    img = img.resize((16,16))
    return np.array(img.getdata())&gt;mean_value

def calculate_simility_by_path(img_path1, img_path2):
    img1 = Image.open(img_path1)
    img2 = Image.open(img_path2)
    fingerprint1 = calculate_fingerprint(img1) 
    fingerprint2 = calculate_fingerprint(img2) 
    return 1 - (0.0+np.sum(np.logical_xor(fingerprint1, fingerprint2)))/fingerprint1.size

if __name__ == "__main__":
    print calculate_simility_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>更好的implementation可以在这个<a href="http://www.ostertag.name/HowTo/findimagedupes.pl">链接</a>找到，虽然做法上不一样，但是原理应该都差不多。在代码的注释里，能看到这个类算法的优点和缺点，如果图片相似度较高的话，识别起来没什么问题。最大的缺点是，很多false positive，如果图片是一张大海的话 会出现一下情况</p>

<pre><code>1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
1111111111111111
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
0000000000000000
</code></pre>

<p>作者还说到对于大规模数据的话，性能会降低很多。但是貌似这个能用bloomfilter解决，现在先不管他</p>
</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai/">高度相似图片检测: 1颜色直方图差</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-08T15:30:00-07:00" pubdate data-updated="true">Sep 8<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-1yan-se-zhi-fang-tu-chai/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>对于颜色直方图差的比对能很好的检测出图片之间的色差变化，那对于两张环境，光线，角度长不错的图片，能很好的检测出他们的相似度。
这是一个很老的方法，简单粗暴，非常使用。原理在这篇<a href="http://blog.csdn.net/lanphaday/article/details/2325027">博客</a>里很清楚的讲解了。 算法很简单:</p>

<ul>
  <li>把两张图片缩放到统一的大小，这里可以是256x256， 并且统一成rgb模式。</li>
  <li>统计每张图片的每个颜色的出现次数。那这里因为是rgb模式，有3*256=768 种颜色，生成直方图</li>
  <li>对两张图片的直方图进行比对。如果统计结果一致的话，相似度+1</li>
</ul>

<p>这里有个问题就是对空间信息的丢失，原作者通过把图片分割成16个小方格来解决这个问题。(split_image函数)</p>

<p>具体代码如下:</p>

<pre><code>from PIL import Image
import sys
def normalize(img, size = (256, 256)):
    return img.resize(size).convert('RGB')

def sim_hist(hist1, hist2):
    assert len(hist1) == len(hist2)
    return sum(1 - (0 if hist1 == hist2 else float(abs(hist1 - hist2))/max(hist1, hist2)) for hist1, hist2 in zip(hist1, hist2))/len(hist1)


def calc_similar_by_path(path_img1, path_img2):
    li = normalize(Image.open(path_img1))
    ri = normalize(Image.open(path_img2))
    return sum(sim_hist(l.histogram(), r.histogram()) for l, r in zip(split_image(li), split_image(ri))) / 16.0



def split_image(img, part_size = (64, 64)):
    w, h = img.size
    pw, ph = part_size
    assert w % pw == h % ph == 0
    return [img.crop((i, j, i+pw, j+ph)).copy() \
        for i in xrange(0, w, pw) \
        for j in xrange(0, h, ph)]

if __name__ == "__main__":
    print calc_similar_by_path(sys.argv[1],sys.argv[2])
</code></pre>

<p>这里的fingerprint就是每个图片的直方图。 这个指纹的信息量的话应该挺大的(至少有2^768) 理论上来讲应该false positive的可能性很小，但是这个结论应该是错的，因为在图片里很多颜色是出现的概率是很小的，大部分颜色都集中于某些值中。</p>

<p>代码和原作者的差不多，实验结果是不错的，很简单，但是这个算法有个致命的问题就是对颜色的过分依赖。对于图片角度的变化的容忍度挺高，但是如果颜色或者图片光线变化稍大就不能很好的检测出相似度了，会把相似的图片判断为不同的。最简单的方法就是弄个灰度图，这方法基于rgb的，就无解了。还有个小问题就是，false positive，我觉得如果两张白底黑字的文字图片，不管字是多么不一样，但是他们的相似度应该很高(在有的应用上，这个结果是很希望的)</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction/">高度相似图片检测: Introduction</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-09-08T14:18:00-07:00" pubdate data-updated="true">Sep 8<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/09/08/gao-du-xiang-si-tu-pian-jian-ce-introduction/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在db实习了2个多月很开心。老实说，在那里做的项目没有想象中的理想，觉得惭愧啊。前段时间清风老师有和我提起一个敏感图片检测的需求。当时候有试着用SIFT和min-hash实现，但是结果不是很理想。在公司和在家自己弄有很大的区别，时间上的紧迫，很多时候虽然头不会催，但是你看着别人在交东西，自己却没有，压力很大，很多时候就需要move on。结果就是这个项目没有完成, 感脚好对不住清风老师啊！真的不会再爱了！</p>

<p>这几天闲下来，我突然有想好好研究研究这方面的东西了，看了些paper，觉得有很多实现方法，结果有好的有坏的，我准备都试着去实现下，把他们的有点和缺点都记录下来，希望能找到一个很好的解决方案，如果运气好的话。</p>

<p>Near-duplicate image(高相似图片)的检测不比完全一样的图片检测来的简单，后者可以直接用哈希生成像MD5类似的指纹，然后保存每个图片的时候也保存那个指纹，这样在查找的时候只要比对指纹就可以了，这样的话速度上会有很大的提升。</p>

<p>那用什么样的办法能有效的比对图片的相似度呢? 方法有很多。 首先要说的一点是，在比对的时候，速度是非常重要的，所以，一般都是通过指纹(fingerprint)技术把一张图片合理的压缩成一个容量占用很小的方便计算相似度的数据集。 前面说过md5是不能用在这里的，为什么呢？因为一个微小的变化会是两个图片之间的MD5完全不一样。而在这里要做的是:</p>

<ul>
  <li>相同图片的指纹要一样</li>
  <li>类似图片的指纹也要类似</li>
  <li>完全不相同的图片指纹的差别很大</li>
</ul>

<p>做到以上几点，那么图片的相似度的识别就完成了，但是要找到一个函数f做到以上这种方法很难。这也是这里要慢慢探索的。</p>

</div>
  
  


    </article>
  
  
    <article>
      
  <header>
    
      <h1 class="entry-title"><a href="/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti/">Tesseract-ocr 中文字符识别速度慢的问题</a></h1>
    
    
      <p class="meta">
        








  


<time datetime="2012-07-08T04:01:00-07:00" pubdate data-updated="true">Jul 8<span>th</span>, 2012</time>
        
         | <a href="/blog/2012/07/08/tesseract-ocr-zhong-wen-zi-fu-shi-bie-su-du-man-de-wen-ti/#disqus_thread">Comments</a>
        
      </p>
    
  </header>


  <div class="entry-content"><p>在做这个ocr项目的时候，首先想到的是用一些开源的项目，通过比较，只有google对中文的支持度算是有好的，通过测试，它的识别度也还是可以。但是会面临到好几个问题,可以察觉到2个主要问题</p>

<ul>
  <li>中文字相比英语字符，识别速度低了好几倍</li>
  <li>中文字符有时候会整行出现大面积的错误，而且非常离谱，甚至有乱码的出现。</li>
</ul>

<p>通过这篇<a href="http://static.googleusercontent.com/external_content/untrusted_dlcp/research.google.com/en/us/pubs/archive/35248.pdf">Adapting the Tesseract Open Source OCR Engine for Multilingual OCR</a>的论文里看，这个项目的框架为了能让各种不同语言的文字进行识别，把中文文字识别以一行句子为切分单位，以文字部首为最基本的识别单位(使用了connected components analysis切分所有不连在一起的部首(也可以用项目的图形debug界面http://code.google.com/p/tesseract-ocr/wiki/ViewerDebugging，能看到中文字是给切分开的)。识别之后，通过best first search对中文字符进行匹配。结果就是，文字识别速度较慢，大部分计算的开销用在了匹配的时候。由于用到了best-first 不能保证全局最优解，往往图像稍微有一些噪音就会会出现大面积的错误的情况，一开始的几个字也许和原本字符相似度还可以，句子后半部的字符就不是了。</p>

<p>这个方法完全是按英语的识别方法，通过以英语单词为基本匹配单位，匹配每个字母成为最有可能的单词。这样的一个方法可以让语言的semantic运用到识别了，大大提高的每个单词的识别度。但是这个方法的本身是特别适合语英语，每个单词的长度不是很大，单词和单词之间有一个明显的空格，可以很好的切分。 但是运用到中文字符识别的时候确出现了以下几个问题:</p>

<ul>
  <li>以google那些人的看法，中文字符之间空隙是很小的，甚至很多时候是字和字之间是连在一起。其实印刷体的中文字符之间的空隙还是很明显的。</li>
  <li>中文句子长度一般都很长，往往匹配的时候需要很多开销</li>
  <li>中文识别的时候，我试过，就算把文本字符之间的空隙分的很大，还是会把一个字的之间的部首分开，在匹配的时候不会记住字符之间的空隙。</li>
</ul>

</div>
  
  


    </article>
  
  <div class="pagination">
    
      <a class="prev" href="/blog/page/2/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
  </div>
</div>
<aside class="sidebar">
  
    
<section>
  <img src="http://gravatar.com/avatar/e67de7fa62d9665c4022a6a065353549" alt="Gravatar image" title="Gravatar Image" />
  <DIV ALIGN=left>I am not good enough for you</DIV>

  <p />
  </section>
  
<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2012/12/27/gao-du-xiang-si-tu-pian-jian-ce-4ji-yu-tu-pian-bu-bian-te-zheng-dian/">高度相似图片检测: 4基于图片不变特征点</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/12/02/guan-yu-knnhe-random-forestsde-gan-jue/">关于Knn和Random Forests的感觉</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/10/19/c-plus-plus-li-shi-yong-octave/">c++里使用octave</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/09/24/fast-approximate-nearest-neighbors-flann/">FAST APPROXIMATE NEAREST NEIGHBORS(FLANN)</a>
      </li>
    
      <li class="post">
        <a href="/blog/2012/09/16/ma-sai-ke-tu-pian-pin-jie/">马赛克图片拼接</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Categories</h1>
      <ul id="category-list"><li><a href='/blog/categories/c-'>C++ (1)</a></li><li><a href='/blog/categories/gabor-filters'>Gabor filters (1)</a></li><li><a href='/blog/categories/google'>Google (1)</a></li><li><a href='/blog/categories/grammatical-evolution'>Grammatical evolution (1)</a></li><li><a href='/blog/categories/knn'>Knn (1)</a></li><li><a href='/blog/categories/knn-'>Knn, (1)</a></li><li><a href='/blog/categories/latex'>Latex (1)</a></li><li><a href='/blog/categories/numpy'>Numpy (1)</a></li><li><a href='/blog/categories/ocr'>Ocr (3)</a></li><li><a href='/blog/categories/octave'>Octave (1)</a></li><li><a href='/blog/categories/octopress'>Octopress (1)</a></li><li><a href='/blog/categories/python'>Python (3)</a></li><li><a href='/blog/categories/scipy'>Scipy (1)</a></li><li><a href='/blog/categories/svm'>Svm (1)</a></li><li><a href='/blog/categories/weka'>Weka (1)</a></li><li><a href='/blog/categories/不平衡数据集'>不平衡数据集 (1)</a></li><li><a href='/blog/categories/中文字图片生成'>中文字图片生成 (1)</a></li><li><a href='/blog/categories/中文字特征提取'>中文字特征提取 (1)</a></li><li><a href='/blog/categories/中文字符识别'>中文字符识别 (1)</a></li><li><a href='/blog/categories/分类'>分类 (4)</a></li><li><a href='/blog/categories/图像'>图像 (7)</a></li><li><a href='/blog/categories/机器学习'>机器学习 (2)</a></li><li><a href='/blog/categories/计算机视觉'>计算机视觉 (7)</a></li><li><a href='/blog/categories/随机森林'>随机森林 (1)</a></li><li><a href='/blog/categories/音乐'>音乐 (1)</a></li><li><a href='/blog/categories/马赛克'>马赛克 (1)</a></li><li><a href='/blog/categories/高度相似图片检测'>高度相似图片检测 (5)</a></li></ul>
      </section>
<section>
最近读的书
<script type="text/javascript" src="http://www.douban.com/service/badge/Rabbby/?show=collection&amp;n=16&amp;columns=4&amp;hidelogo=yes&amp;cat=book|site" ></script>
</section>


<section>
my Last.fm
<a href="http://www.last.fm/user/kkx123456/?chartstyle=basic10"><img src="http://imagegen.last.fm/basic10/otracks/kkx123456.gif" border="0" /></a>
</section>






  
</aside>

    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - 小逸 -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  

<script type="text/javascript">
      var disqus_shortname = 'kkx';
      
        
        var disqus_script = 'count.js';
      
    (function () {
      var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
      dsq.src = 'http://' + disqus_shortname + '.disqus.com/' + disqus_script;
      (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    }());
</script>



<div id="fb-root"></div>
<script>(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#appId=212934732101925&xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));</script>



  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>



  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
